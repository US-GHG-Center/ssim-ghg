{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a514734-7628-4132-b4cb-25a683a07426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from random import choices\n",
    "from glob import glob\n",
    "import xarray as xr\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c8661-6653-4faf-8ac7-47ae92bf83d5",
   "metadata": {},
   "source": [
    "# **Inverse Problems Primer, continued.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee4fd3-0fc3-4ca8-9603-434cc0afffcc",
   "metadata": {},
   "source": [
    "# II. Inverse Problem Formulation 2: Bayesian Probabilistic Approach\n",
    "## A more general approach for the inverse problem formulation is to think about the observations (or equivalently, their errors) and forward model parameters as distributions. We implicitly did this for the observations in the previous example when we created the noise: \n",
    "### `noise = bias + scatter*np.random.randn(n_obs)`\n",
    "### This represents sampling a normal distribution with mean $\\mu = $ `bias` and standard deviation $\\sigma = $ `scatter`. However, the optimal estimation approach with real observations does not require any assumptions about the distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f75d7-a460-450a-9ce8-de5bd86b94c0",
   "metadata": {},
   "source": [
    "## II.A. The Bayesian Problem Setup\n",
    "### Recall Bayes' Theorem: $$ f_{B|Y=y}(\\beta) \\propto f_{Y|B=\\beta}(y)f_B(\\beta). $$ This equation is true for all distributions, but we interpret it in the context of our problem:\n",
    "### [1] $B$ is the distribution of the parameters we want to infer, which we assign a prior distribution with density $f_B$.\n",
    "### [2] $Y$ is the distribution of observations (or equivalently, their errors).\n",
    "### [3] The function $f_{Y|B=\\beta}$ is called the \"likelihood\" of Y taking a value of $y$ when $B=\\beta$.\n",
    "### [4] The density $f_{B|Y=y}$ describes the \"posterior\" distribution of $B$,\n",
    "\n",
    "### In order to fully specify our problem, we have to specify the prior density $f_B$ and likelihood function $f_{Y|B=\\beta}$. The models we specify encode our assumptions about 1) the quality of the observations $\\hat{y}$, 2) the quality of our mapping between $\\beta$ and $y$, and 3) our best estimate of our uncertainty in the parameters $\\beta$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714d2f5-fa18-45c6-83b1-426470b95748",
   "metadata": {},
   "source": [
    "## II.B The Gaussian Assumption\n",
    "### A typical approach (and one that is complementary to optimal estimation) is to assume Gaussian functions for our ingredients (ignoring normalization constants for now):\n",
    "### [*Prior*] $$f_B(\\beta) = \\exp\\left[-(\\beta-\\beta_0)^{\\mathsf{T}}\\Sigma^{-1}_{B}(\\beta-\\beta_0)\\right]$$\n",
    "### [*Likelihood*] $$f_{Y|B=\\beta}(y) = \\exp\\left[-(y-F(x;\\beta))^{\\mathsf{T}}\\Sigma^{-1}_{R}(y-F(x;\\beta))\\right]$$\n",
    "### This leads to the posterior parameter density function:\n",
    "### [*Posterior*] $$ f_{B|Y=y}(\\beta) = \\exp\\left[-(y-F(x;\\beta))^{\\mathsf{T}}\\Sigma^{-1}_{R}(y-F(x;\\beta))-(\\beta-\\beta_0)^{\\mathsf{T}}\\Sigma^{-1}_{B}(\\beta-\\beta_0)\\right]$$\n",
    "\n",
    "### *Connection to least-squares*: Note that $$\\log f_{B|Y=y}(\\beta) = -|y-F(x;\\beta)|_{\\Sigma_R^{-1}} - |\\beta-\\beta_0|_{\\Sigma_B^{-1}}.$$ This tells us that when we model the prior and likelihood with Gaussian distributions, the mode of the posterior $f_{B|Y=y}$ is also the minimizer of the weighted least squares cost function $$ J(\\beta) = (y-F(x;\\beta))^{\\mathsf{T}}\\Sigma^{-1}_{R}(y-F(x;\\beta)) + (\\beta-\\beta_0)^{\\mathsf{T}}\\Sigma^{-1}_{B}(\\beta-\\beta_0) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35c657-0ac3-4635-9a8d-59b00375f323",
   "metadata": {},
   "source": [
    "## II.C Exploration with Gaussian Models\n",
    "### [1] Data is a combination of model simulation from \"true\" parameters and unbiased Gaussian noise\n",
    "### [2] Prior distribution is Gaussian with incorrect mean and uncorrelated covariances.\n",
    "### [3] Likelihood is a Gaussian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d404d-4b97-4a99-bba7-7943a427c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1)\n",
    "n_obs = 100\n",
    "\n",
    "# Noise is normally distributed with a bias and scatter\n",
    "bias = 0.\n",
    "scatter = 10.\n",
    "noise = bias + scatter*np.random.randn(n_obs)\n",
    "sig_R = np.diag(scatter**2*np.ones(n_obs))\n",
    "sig_R_inv = np.diag(1/scatter**2*np.ones(n_obs))\n",
    "\n",
    "# Model looks like y = -5x_1 + 5x_2\n",
    "bet = np.array([-5,5])\n",
    "\n",
    "# Create truth observations \n",
    "x1 = 10*np.random.randn(n_obs)\n",
    "x2 = 10*np.random.randn(n_obs)\n",
    "X = np.c_[(x1,x2)]\n",
    "y = np.dot(X,bet)   #\"truth\" model output\n",
    "yhat = y + noise     # noisy data\n",
    "\n",
    "# Create Gaussian prior distribution for beta_1, beta_2 parameters\n",
    "b1,b2 = np.mgrid[-10:0:.1, 0:10:.1]\n",
    "pos = np.dstack((b1, b2))\n",
    "bet_prior = [-4,6]\n",
    "sig_B = [[1,0],[0,4]]\n",
    "pdf_prior = multivariate_normal(bet_prior,sig_B).pdf(pos)\n",
    "y_prior = np.dot(X,bet_prior)\n",
    "\n",
    "#Likelihood based on Gaussian assumption above\n",
    "likelihood = np.zeros(b1.shape)\n",
    "for ii in range(b1.shape[0]):\n",
    "    for jj in range(b1.shape[1]):\n",
    "        mdm = yhat-np.dot(X,np.array([b1[ii,jj],b2[ii,jj]]))\n",
    "        likelihood[ii,jj] = np.exp(-np.dot(mdm,np.dot(sig_R_inv,mdm)))\n",
    "\n",
    "#Posterior is the normalized product of the likelihood and prior\n",
    "pdf_post = pdf_prior*likelihood\n",
    "pdf_post /= np.nansum(pdf_post)*0.1**2\n",
    "\n",
    "#Mode of the posterior and simulated obs\n",
    "bet_post = b1.flatten()[np.argmax(pdf_post)],b2.flatten()[np.argmax(pdf_post)]\n",
    "y_post = np.dot(X,bet_post)\n",
    "\n",
    "#Plot the prior and posterior mismatch with simulated data\n",
    "plt.hist(np.c_[y_prior-yhat,y_post-yhat],label=['Prior Mismatch','Posterior Mismatch'])\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel(r'$\\hat{y}-y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887dd4d-cc07-4ce7-8fe7-6d4f0dccccba",
   "metadata": {},
   "source": [
    "### **Questions**\n",
    "### [1] How does the prior fit to the data compare with the posterior fit to the data?\n",
    "### [2] Why aren't the posterior mismatches zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3437b5e-9804-4013-b42d-8d1be515e509",
   "metadata": {},
   "source": [
    "## Let's explore the components of the Bayes' architecture.\n",
    "### *The next sets of plots visualize the functions that make up the prior density, likelihood, and posterior density functions, both as 2D images as well as the marginal versions, in order to show how including data affects our knowledge of the parameters.*\n",
    "### Prior Joint Density and Marginal Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d1ab1-ec25-440e-ae26-8636df78528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1,2.5], height_ratios=[3,1])\n",
    "ax = plt.subplot(gs[0,1])\n",
    "axl = plt.subplot(gs[0,0], sharey=ax)\n",
    "axb = plt.subplot(gs[1,1], sharex=ax)\n",
    "\n",
    "g = ax.contourf(b1,b2,pdf_prior,cmap=plt.cm.hot_r)\n",
    "ax.plot(*bet,'*',ms=10)\n",
    "ax.set_title('Joint Prior Density for $\\\\beta_1$ and $\\\\beta_2$')\n",
    "\n",
    "p1 = axl.plot(pdf_prior.sum(1)*0.1,b2.mean(0),'tab:orange')\n",
    "p2 = axl.axhline(bet[1])\n",
    "axl.legend([p1[0],p2],['Marginal','Truth'])\n",
    "axl.set_ylabel('Parameter $\\\\beta_2$')\n",
    "\n",
    "p1 = axb.plot(b1.mean(1),pdf_prior.sum(0)*0.1,'tab:orange')\n",
    "p2 = axb.axvline(bet[0])\n",
    "axb.legend([p1[0],p2],['Marginal','Truth'])\n",
    "axb.set_xlabel('Parameter $\\\\beta_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57fa23a-e00b-4890-89d3-d9b49b8ca96c",
   "metadata": {},
   "source": [
    "### Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d2211-d0e1-430c-b293-e22c4988469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1,2.5], height_ratios=[3,1])\n",
    "ax = plt.subplot(gs[0,1])\n",
    "axl = plt.subplot(gs[0,0], sharey=ax)\n",
    "axb = plt.subplot(gs[1,1], sharex=ax)\n",
    "\n",
    "g = ax.contourf(b1,b2,np.log(likelihood),cmap=plt.cm.hot_r)\n",
    "ax.plot(*bet,'*',ms=10)\n",
    "ax.set_title('Log Likelihood($\\\\beta_1$,$\\\\beta_2$)')\n",
    "\n",
    "p1 = axl.plot(np.log(likelihood.sum(1)),b2.mean(0),'tab:orange')\n",
    "p2 = axl.axhline(bet[1])\n",
    "axl.legend([p1[0],p2],['Marginal','Truth'])\n",
    "axl.set_ylabel('Parameter $\\\\beta_2$')\n",
    "\n",
    "p1 = axb.plot(b1.mean(1),np.log(likelihood.sum(0)),'tab:orange')\n",
    "p2 = axb.axvline(bet[0])\n",
    "axb.legend([p1[0],p2],['Marginal','Truth'])\n",
    "axb.set_xlabel('Parameter $\\\\beta_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad75a9-69ba-486d-911c-4719e57e5d9e",
   "metadata": {},
   "source": [
    "### Joint Posterior Density and Marginal Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ef77b-5f01-4b03-bc15-97ee6ac9abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gridspec.GridSpec(2, 2, width_ratios=[1,2.5], height_ratios=[3,1])\n",
    "ax = plt.subplot(gs[0,1])\n",
    "axl = plt.subplot(gs[0,0], sharey=ax)\n",
    "axb = plt.subplot(gs[1,1], sharex=ax)\n",
    "\n",
    "g = ax.contourf(b1,b2,pdf_post,cmap=plt.cm.hot_r)\n",
    "ax.plot(*bet,'*',ms=10)\n",
    "ax.set_title('Joint Posterior Density for $\\\\beta_1$ and $\\\\beta_2$')\n",
    "\n",
    "p1 = axl.plot(pdf_post.sum(1)*0.1,b2.mean(0),'tab:orange')\n",
    "p2 = axl.axhline(bet[1])\n",
    "axl.legend([p1[0],p2],['Marginal','Truth'])\n",
    "axl.set_ylabel('Parameter $\\\\beta_2$')\n",
    "\n",
    "p1 = axb.plot(b1.mean(1),pdf_post.sum(0)*0.1,'tab:orange')\n",
    "p2 = axb.axvline(bet[0])\n",
    "axb.legend([p1[0],p2],['Marginal','Truth'])\n",
    "axb.set_xlabel('Parameter $\\\\beta_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221077c0-8645-4fc9-b623-4da19a1dc60a",
   "metadata": {},
   "source": [
    "### Comparison of Prior and Posterior Marginal Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176f9ec-2a78-4a75-9a76-1b35022a0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(10,4))\n",
    "\n",
    "ax = axs[0]\n",
    "p1 = ax.plot(b1.mean(1),pdf_prior.sum(0)*0.1,'tab:orange')\n",
    "p2 = ax.plot(b1.mean(1),pdf_post.sum(0)*0.1,'tab:green')\n",
    "p3 = ax.axvline(bet[0])\n",
    "ax.legend([p1[0],p2[0],p3],['Prior','Posterior','Truth'])\n",
    "ax.set_xlabel('Parameter $\\\\beta_1$');\n",
    "ax.set_title('Marginal Prior and Posterior Densities for $\\\\beta_1$')\n",
    "\n",
    "ax = axs[1]\n",
    "p1 = ax.plot(pdf_prior.sum(1)*0.1,b2.mean(0),'tab:orange')\n",
    "p2 = ax.plot(pdf_post.sum(1)*0.1,b2.mean(0),'tab:green')\n",
    "p3 = ax.axhline(bet[1])\n",
    "ax.legend([p1[0],p2[0],p3],['Prior','Posterior','Truth'])\n",
    "ax.set_ylabel('Parameter $\\\\beta_2$');\n",
    "ax.set_title('Marginal Prior and Posterior Densities for $\\\\beta_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8ae25-fdb3-4834-b7b5-d696b3186fc9",
   "metadata": {},
   "source": [
    "### **Exercises**\n",
    "### [1] Compute the mean parameters $\\beta_1$ and $\\beta_2$ for the prior and posterior. Recall that the mean for each parameter is the mean of the marginal density.\n",
    "### [2] Compute the standard deviation of each marginal distribution for the prior and posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35162d-2b5a-43cd-bd83-eb69c5e5da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the mean parameters\n",
    "\n",
    "### Compute the marginal standard deviations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834ecb1-afca-4961-94d3-7e4994ecd96c",
   "metadata": {},
   "source": [
    "### **Questions**\n",
    "### [1] How does the prior mean $\\beta$ differ from the posterior mean $\\beta$?\n",
    "### [2] How does the prior spread differ from the posterior spread? \n",
    "### [3] How do we interpret these differences as \"knowledge\" on the parameters $\\beta_1$ and $\\beta_2$? Did inclusion of observations improve our understanding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a1e62-ea4c-43a0-a14b-ab01c3846f37",
   "metadata": {},
   "source": [
    "## Critical Thinking\n",
    "### There were many assumptions made during this exercise. How might these results change if you changed the mean or covariance of the prior density function or picked a different family of density entirely? How about if the observations contained a nonzero bias that was not accounted for in our model?\n",
    "\n",
    "### What happens if the forward model used in the likelihood calculation is different from the \"true\" model we used to generate the data? Perhaps the true relationship is nonlinear, but we often assume the forward model is linear in the absence of other information. \n",
    "\n",
    "### *Feel free to copy and paste the code below and do your own experiments!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4468e-c9d5-4a8b-a22f-fedec220a943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
