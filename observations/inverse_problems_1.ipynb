{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a514734-7628-4132-b4cb-25a683a07426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats,special\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from random import choices\n",
    "from glob import glob\n",
    "import xarray as xr\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c8661-6653-4faf-8ac7-47ae92bf83d5",
   "metadata": {},
   "source": [
    "# **Inverse Problems Primer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1de773-1d02-4d3d-b451-aeeb54978eb6",
   "metadata": {},
   "source": [
    "\n",
    "#### First, a familiar example. Consider a multivariate linear model where we try to predict a variable $y$ with a linear combination of variables $x = [x_i]_{i=1}^{n}$ and potentially an intercept: $$y = \\beta_0 + \\sum_{i=1}^{n} \\beta_i x_i = F(x).$$ We usually think about this type of model with the $\\beta$ already specified, but we can also expand our thinking to include the entire family of such models $F(x;\\beta)$, where \"$;\\beta$\" notation is typical for parameters, but really just means $F$ is a function of both $x$ and $\\beta$.\n",
    "\n",
    "#### In the case that we have some data $\\hat{y}$ with predictor samples $\\hat{x}$, we can think of $F(x;\\beta)$ as the forward model from the space of predictors $x$ and parameters $\\beta$. If we specify $\\beta$, then we have a specific multilinear model that we can compare against data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcdcdb-7c9e-4060-990e-a10b71cc732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Parameters\n",
    "beta_0 = 0\n",
    "beta_1 = 1\n",
    "x = np.linspace(-10,10,21)\n",
    "y = beta_0 + beta_1*x\n",
    "noise = np.random.randn(len(x))\n",
    "yhat = y + noise\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,yhat,'o')\n",
    "plt.legend(['True Mapping','Simulated Noisy Data $\\hat{y}$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efde7fd-3eb1-4838-a619-4ecc3d47441b",
   "metadata": {},
   "source": [
    "### **Exercise** \n",
    "#### 1. Copy and paste the code below and modify it for the case of noise with a bias of 1 and standard deviation 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b41bd-ea51-4f9e-b85a-f30a373940f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60ee4fd3-0fc3-4ca8-9603-434cc0afffcc",
   "metadata": {},
   "source": [
    "# **II. Inverse Problems as Optimal Estimation**\n",
    "# II.A Distance as an Error Metric\n",
    "### In the real world, we have $m$ observations of a quantity we want to predict $$\\hat{y}= [\\hat{y}_i]_{i=1}^{m}$$ organized as a vector and also $m$ observations of $n$ predictors organized as a $m\\times n$ matrix $\\hat{X} = [x_{ij}]_{i,j=1}^{m,n}$. We would like to use these data to hypothesize a model $F$ for the general mapping $x\\rightarrow y$. \n",
    "\n",
    "### In our simple example we would likely guess a linear model $$ y = F(x;\\beta) = \\beta_0 + \\beta_1 x.$$ With these assumptions, we can ask what parameters $\\beta$ would result in the \"best\" forward model $F(x;\\beta).$ \n",
    "\n",
    "### We define \"best\" meaning minimizing a **chosen distance** between  and $y=[y_i] = F([x_{i,:}];\\beta)]_{i=1}^{m}$. Some typical choices are:\n",
    "### $\\cdot$ $\\mathcal{L}^1$: $|\\hat{y}-y|_1 = \\sum_{i=1}^{m}|\\hat{y}_i - y_i|$\n",
    "### $\\cdot$ $\\mathcal{L}^2$: $|\\hat{y}-y|_2 = \\sqrt{\\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2} = \\sqrt{(\\hat{y}-y)^{\\mathsf{T}}(\\hat{y}-y)}$ (i.e., the Euclidean distance, or the distance derived from the standard dot product of two vectors)\n",
    "### $\\cdot$ $\\mathcal{L}^\\infty$: $|\\hat{y}-y|_\\infty=\\max_{i=1,...,m}|\\hat{y}_i - y_i|$\n",
    "### $\\cdot$ $\\mathcal{L}^2_W$:  $|\\hat{y}-y|_W = \\sqrt{(\\hat{y}-y)^{\\mathsf{T}}W(\\hat{y}-y)}$ \n",
    "### In this last case, $W$ is a positive definite matrix that we think of as weighting the individual mismatches $\\hat{y}_i-y_i$. \n",
    "### The code below computes these different error metrics for a randomly selected parameter vector $\\beta$ and plots histograms.\n",
    "#### *Extra Credit: why do the different metrics have the relative sizes evident in the histograms?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d404d-4b97-4a99-bba7-7943a427c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noisy data\n",
    "beta_0 = 0\n",
    "beta_1 = 1\n",
    "x = np.linspace(-10,10,21)\n",
    "y = beta_0 + beta_1*x    #\"truth\" model output\n",
    "noise = np.random.randn(len(x))\n",
    "yhat = y + noise     # noisy data\n",
    "\n",
    "# Error computation for our toy example\n",
    "beta_0_test = np.random.randn(1000)\n",
    "beta_1_test = np.random.randn(1000)+1\n",
    "resids = np.array([yhat-beta_0_test[i]-beta_1_test[i]*x for i in range(1000)])\n",
    "l1_errs = np.abs(resids).sum((1))\n",
    "l2_errs = np.sqrt((resids**2).sum((1)))\n",
    "linf_errs = np.max(np.abs(resids),(1))\n",
    "plt.hist(l1_errs,density=True)\n",
    "plt.hist(l2_errs,density=True,alpha=0.8)\n",
    "plt.hist(linf_errs,density=True,alpha=0.5)\n",
    "plt.legend(['$\\mathcal{L}^1$','$\\mathcal{L}^2$','$\\mathcal{L}^\\infty$'])\n",
    "plt.title('Error Comparison for Randomly Selected $ \\\\beta$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1f56a-9804-4b66-a42a-e8a58d0d5468",
   "metadata": {},
   "source": [
    "### The following plots show the $\\mathcal{L}^2$ errors with a random sample of the intercept $\\beta_0$ and $\\beta_1$ parameters (1) in 2-D space, (2) in 1-D with fixed values of $\\beta_1$, and (3) in 1-D with fixed values of $\\beta_0.$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1533f-00ac-4f81-a86d-a6af3ba0630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the errors with the beta values varying independently\n",
    "beta_0_sub = sorted(beta_0_test[::10])\n",
    "beta_1_sub = sorted(beta_1_test[::10])\n",
    "resids = np.array([[yhat-beta_0_sub[i]-beta_1_sub[j]*x for i in range(len(beta_0_sub))] for j in range(len(beta_1_sub))])\n",
    "l1_errs = np.abs(resids).sum((2))\n",
    "l2_errs = np.sqrt((resids**2).sum((2)))\n",
    "linf_errs = np.max(np.abs(resids),(2))\n",
    "\n",
    "beta_0_test_2d,beta_1_test_2d = np.meshgrid(beta_0_sub,beta_1_sub)\n",
    "df = pd.DataFrame({'beta_0':beta_0_test_2d.flatten(),'beta_1':beta_1_test_2d.flatten(),'l1_errs':l1_errs.flatten(),'l2_errs':l2_errs.flatten(),'linf_errs':linf_errs.flatten()})\n",
    "\n",
    "# Now make the plots!\n",
    "fig,axs = plt.subplots(1,3,figsize=(16,4))\n",
    "df.plot.scatter(x='beta_0',y='beta_1',c='l2_errs',ax=axs[0])\n",
    "\n",
    "#Plot a few slices with beta_1 constant\n",
    "ax = axs[1]\n",
    "g1 = ax.plot(beta_0_sub,l2_errs[0,:],'.')\n",
    "g2 = ax.plot(beta_0_sub,l2_errs[50,:],'.')\n",
    "g3 = ax.plot(beta_0_sub,l2_errs[-1,:],'.')\n",
    "ax.legend([g1[0],g2[0],g3[0]],[f'$\\\\beta_1=${beta_1_sub[20]:4.2f}',f'$\\\\beta_1=${beta_1_sub[50]:4.2f}',f'$\\\\beta_1=${beta_1_sub[75]:4.2f}'])\n",
    "ax.set_xlabel('Intercept Parameter $\\\\beta_0$')\n",
    "ax.set_ylabel(r'$\\mathcal{L}_2$ Error')\n",
    "ax.set_title(r'$\\mathcal{L}^2$ Error versus Intercept Parameter')\n",
    "\n",
    "ax = axs[2]\n",
    "g1 = ax.plot(l2_errs[:,0],beta_1_sub,'.')\n",
    "g2 = ax.plot(l2_errs[:,50],beta_1_sub,'.')\n",
    "g3 = ax.plot(l2_errs[:,-1],beta_1_sub,'.')\n",
    "ax.legend([g1[0],g2[0],g3[0]],[f'$\\\\beta_0=${beta_0_sub[20]:4.2f}',f'$\\\\beta_0=${beta_0_sub[50]:4.2f}',f'$\\\\beta_0=${beta_0_sub[75]:4.2f}'])\n",
    "ax.set_ylabel('Slope Parameter $\\\\beta_1$')\n",
    "ax.set_xlabel(r'$\\mathcal{L}_2$ Error')\n",
    "ax.set_title(r'$\\mathcal{L}^2$ Error versus Slope Parameter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daa822-5671-4a2b-96ef-5e89f907e06b",
   "metadata": {},
   "source": [
    "### Questions\n",
    "#### 1. How does the $\\mathcal{L}^2$ error vary in each parameter? Is it linear, quadratic, something else? Is it the same for both parameters? \n",
    "#### 2. Does the $\\mathcal{L}^2$ error as a function of $\\beta_0$ and $\\beta_1$ have a minimum? Where? Why is that intuitive given how the simulated observations were created?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252b3b5-54a1-4fbc-b419-cdae977f2328",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "#### 1. Repeat the analysis and answer the questions for the $\\mathcal{L}^1$ errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9b36a-8169-4fc8-b09c-7672e262ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L^1 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a853b86-c1f5-4939-bb12-0524fa243b72",
   "metadata": {},
   "source": [
    "#### 2. Repeat the analysis and answer the questions for the $\\mathcal{L}^\\infty$ errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb888649-1568-407f-b0b9-e9e73c146a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24d39803-c22b-42d6-8c33-8eef3ece0faa",
   "metadata": {},
   "source": [
    "# II.B. Minimization of Distance\n",
    "### Given a set of observations $(\\hat{X},\\hat{y})$, a model $y_\\beta = F(x;\\beta)$, and an error metric $|\\cdot|$, we can ask the question:\n",
    "### <center> IP1: *What parameter vector $\\beta^*$ minimizes $|\\hat{y}-y_\\beta|$*? </center>\n",
    "### This formulation intuitively chooses the best model $F(\\hat{x};\\beta^*)$ to reproduce the data $\\hat{y}$ given our definition of distance. Sticking with our multilinear model example above (assuming no intercept): $$y = [F(x_i;\\beta)]_{i=1}^{m} = \\left[\\sum_{j=1}^{n}\\beta_j x_{ij}\\right]_{i=1}^{m} = \\hat{X}\\beta$$ and the distance (or cost) $$ J(\\beta) = |\\hat{y}-y|_2^2 = (\\hat{y}-\\hat{X}\\beta)^{\\mathsf{T}}(\\hat{y}-\\hat{X}\\beta), $$ which is typically called the \"least squares cost function\". Remembering our college algebra and calculus, we can find the minimium of this paraboloid when the gradient is 0: $$0 = \\nabla J = \\left[\\frac{\\partial J}{\\partial \\beta_i}\\right]_{i=1}^{n} = -2 \\hat{X}^{\\mathsf{T}}\\hat{y} + 2\\hat{X}^{\\mathsf{T}}\\hat{X}\\beta$$ or $$ \\hat{X}^{\\mathsf{T}}\\hat{X}\\beta = \\hat{X}^{\\mathsf{T}}\\hat{y} $$ So now we have a typical linear algebra problem where we want to find $\\beta$ that satisfies the $m$ linear equations. Remember that this problem could be overdetermined $(m < n)$ or underdetermined $(m > n)$ and that even if $m = n$ we can still have issues when $\\hat{X}^{\\mathsf{T}}\\hat{X}$ is not invertible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199eb4bc-8247-4e2e-aa45-0a294b452ba1",
   "metadata": {},
   "source": [
    "## *Example*: Let's try an example where we generate some data from a secret \"truth\" $\\beta$ and try to find the optimal solution $\\beta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ad21b-6350-4591-a509-0bbf100d2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 1000\n",
    "\n",
    "# Noise is normally distributed with a bias and scatter\n",
    "bias = 0.\n",
    "scatter = 10.\n",
    "noise = 0.#bias + scatter*np.random.randn(n_obs)\n",
    "\n",
    "# Model looks like y = -5x_1 + 5x_2\n",
    "bet = np.array([-5,5])\n",
    "\n",
    "# Create truth observations \n",
    "x1 = 10*np.random.randn(n_obs)\n",
    "x2 = 10*np.random.randn(n_obs)\n",
    "X = np.c_[(x1,x2)]\n",
    "y = np.dot(X,bet)   #\"truth\" model output\n",
    "yhat = y + noise     # noisy data\n",
    "\n",
    "# Find the optimal parameters\n",
    "xtx = np.dot(X.T,X)\n",
    "ytx = np.dot(yhat.T,X)\n",
    "beta_star = np.dot(np.linalg.inv(xtx),ytx)\n",
    "\n",
    "# Compute the prediction error\n",
    "y_star = np.dot(X,beta_star)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(x1,x2,yhat,c=yhat)\n",
    "ax.set_xlabel(r'$x_1$',fontsize=18)\n",
    "ax.set_ylabel(r'$x_2$',fontsize=18)\n",
    "ax.set_zlabel(r'$\\hat{y}$',fontsize=18)\n",
    "ax.set_title('Sample Observations')\n",
    "\n",
    "print(f'Optimal Model: y = {beta_star[0]:4.2f} x_1 + {beta_star[1]:4.2f} x_2')\n",
    "print(f'True Model: y = {bet[0]:4.2f} x_1 + {bet[1]:4.2f} x_2')\n",
    "print(f'Errors: beta_1 = {(beta_star[0]-bet[0])/bet[0]*100:4.2f}%, beta_2 = {(beta_star[1]-bet[1])/bet[1]*100:4.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6c39d-502e-46a6-8c21-5642cdfd13ad",
   "metadata": {},
   "source": [
    "### Success! We were able to exactly recover the \"true\" parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c87d6-d1eb-4aa4-a8fa-d1624d5adb98",
   "metadata": {},
   "source": [
    "# II.C Noise and Uncertainty\n",
    "### We saw in the last example that in the case of perfect data with sufficiently few parameters, we can exactly recover the true parameters. \n",
    "### Real observational data always contains uncertainty due to the limits of measurement technology and human operators. It is natural to ask how these uncertainties will affect our ability to estimate parameters, and what the uncertainty in our estimates of those parameters will be. Scientific understanding demands not just the optimal estimate $\\beta^*$, but rather some notion of the uncertainty in the estimate. \n",
    "\n",
    "### In the simplest case, we assume that $$ y = X\\beta + \\epsilon $$ where $\\epsilon$ is a random variable that captures the uncertainty in our ability to reproduce the behavior of $y$ with the model $F$, whether that is due to observational noise or model error. In order to proceed, we have to assume something about $\\epsilon$, and it's typical to assume that $\\epsilon \\sim N(0,\\Sigma)$, i.e., errors are normally distributed with zero bias and covariance $E\\{\\epsilon\\epsilon^{\\mathsf{T}}\\} = \\Sigma$. \n",
    "\n",
    "### Since we now have random variables, we seek to minimize the **expectation** of the square error: $$ E\\{(\\hat{y}-F(x;\\beta))^{\\mathsf{T}}(\\hat{y}-F(x;\\beta))\\} = E\\{|\\hat{y}-F(x;\\beta)|^2\\} $$\n",
    "\n",
    "### If we assume that the optimal estimate $\\beta^*$ is unbiased, i.e., $E\\{\\beta^*\\} = \\hat{\\beta}$, then we get the result that the point estimate $$ \\beta^* = (X^\\mathsf{T}\\Sigma X)^{-1}(X^{\\mathsf{T}}\\Sigma^{-1}\\hat{y}) $$ and that the corresponding error covariance is $$ E\\{|\\beta^*-\\hat{\\beta}|^2\\} =  (X^\\mathsf{T}\\Sigma X)^{-1} $$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce8388-2807-4161-9339-f0802baa1acd",
   "metadata": {},
   "source": [
    "### These are the errors in the simulated observations from the forward model with the parameter estimate $\\beta^*$. \n",
    "## **Questions:**\n",
    "### [1] What do you think might make the parameter estimate better? What might make it worse? \n",
    "### [2] Is the distribution of mismatches $\\hat{y}-y^*$ a normal distribution? \n",
    "### [3] Is the distribution of mismatches biased? \n",
    "### [4] What is the scatter of the distribution? How is that tied to the assumptions we made? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45281f7d-38d6-4630-87dc-803ba3f89f6e",
   "metadata": {},
   "source": [
    "## **Exercises**\n",
    "### [1] Copy and paste the code above below and reduce the number of observations to 100. How does that change the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d711ad8-7f2a-4c49-9ed8-a44b04cdf262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9584a8f5-7360-4501-8794-d3a1266462b5",
   "metadata": {},
   "source": [
    "### [2] Copy and paste the code above to the box below and add a bias of 5 to the noise. How does that change the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd378f75-e477-49c2-a5b2-a4f3881b6ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0160c4-077a-4d51-a45e-018c9dffcf24",
   "metadata": {},
   "source": [
    "## **Critical Thinking**\n",
    "### In the simple example above, the observations were generated with the same model family we used in the optimization. What would happen if those two model families weren't the same? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406c2cb-ecd5-4f4c-89b1-0c23faea3227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
