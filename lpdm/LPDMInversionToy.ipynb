{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc1aaae-02be-4fab-b2dc-459ea976429f",
   "metadata": {},
   "source": [
    "## Research Question: \n",
    "### What are the February and July fossil CO<sub>2</sub> estimates for Baltimore in 2019?  Is our understanding of Baltimore's CO<sub>2</sub> emissions consistent with atmospheric observations.   \n",
    "\n",
    "## Method: \n",
    "### Use an atmospheric inversion to (1) estimate emissions and uncertainties, and (2) compare to prior emissions at the monthly scale.  \n",
    "- We are going to run different exercises **(OSSEs)**.  You should always run an OSSE to test your code before running a real data case.\n",
    "- This toy code has a series of blocks that opens libraries, and files along with all the plotting and vector/df/matrix rearranging.\n",
    "- Assume that biospheric emissions are perfectly well known. \n",
    "- Assumes that you already computed your Jacobian **H** in the right format.\n",
    "- Assumes that you have observations and that they have already been subsetted for afternoon hours.    \n",
    "- The toy code creates all the pieces of the inversion and provides output like statistics, plots, etc.\n",
    "- For the different OSSEs, we are going to use a \"true emission\" and \"prior emissions\".  We will also vary how we parameterize **S<sub>z** and **S<sub>o**.  We have two choices for emissions: ACES_FFDAS (Gately et al., ) and GRA2PES (website).\n",
    "- **Note**, please wait until you see the word **\"done!\"** before moving to the next block of code.  Some code blocks take a minute or two to run.\n",
    "\n",
    "### Remember that we use OSSEs to (1) test your code, (2) explore sensitivities, and (3) find the best inversion setup for your situation.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "The code block below loads necessary python libraries and some files.  This is administrative stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06648ab6-9f28-4456-911e-490cfbe5b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy Code for Lagrangian Local-Scale Problems Using In-Situ Data from 3 Fixed Sites Around Baltimore\n",
    "#Written by Kim Mueller (Kimlm@umich.edu)\n",
    "#For GHG Center Summer School\n",
    "#Date July 2025\n",
    "#NOTE - for teaching purposes only.  Do not use outside of classroom, not responsible for any errors.\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:/Users/klm3/AppData/Local/Programs/Python/Python311/Lib/site-packages')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy.sparse import diags\n",
    "from scipy.linalg import inv as dense_inv\n",
    "from scipy.linalg import cholesky \n",
    "import scipy.sparse as sp\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import Polygon, Point, box\n",
    "from matplotlib.colors import Normalize, LogNorm \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mask_df = pd.read_csv('G:/SummerSchool/shapefiles/mask.csv')\n",
    "mask_array = mask_df['Mask'].values\n",
    "num_fluxes =sum(mask_array==1)\n",
    " \n",
    "#add lons and lats\n",
    "subdir = 'priors_processed'\n",
    "priorstring = 'ACES_FFDAS'  \n",
    "lats = np.load('G:/SummerSchool/priors/'+subdir+'/'+priorstring+'_lat.npy')\n",
    "lons = np.load('G:/SummerSchool/priors/'+subdir+'/'+priorstring+'_long.npy')\n",
    "lons = lons[mask_array.ravel().astype(bool)] #removes rows where mask_array = 0\n",
    "lats = lats[mask_array.ravel().astype(bool)]    \n",
    "lat_grid = np.unique(lats)\n",
    "lon_grid = np.unique(lons)\n",
    "mask = True\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92216f6-cdf9-4bc1-bc52-15c9fbec8b5b",
   "metadata": {},
   "source": [
    "## Break into groups to run one of the six exercises (specified below) using the toy code\n",
    "------------------------------------------------------------------------\n",
    "### Example 1:  \n",
    "### Ensure your inversion code is working correctly (base case)\n",
    "\n",
    "#### To do this \n",
    "(1) Create enhancement (**H${x}$<sub>o**) using **H** (created with WRF2-STILT) and **${x}$<sub>o** (ACES+FFDAS) which we assume is **z<sub>truth**.   (2) Use **z<sub>truth** with **H** (created with with WRF2-STILT), and **${x}$<sub>o** (ACES+FFDAS) to estimate $\\hat{x}$.    \n",
    "(3) Use a diagonal of ones multiplied by 0.001 for **S<sub>z**.    \n",
    "(4) Use a diagonal of ones for **S<sub>o**. \n",
    "\n",
    "Please answer the following:  \n",
    "- Are you able to retrieve the truth?\n",
    "- Why did we construct OSSE like this?\n",
    "- Do you have any other insights?\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### Example 2:  \n",
    "### Adding varying white noise on **z<sub>truth** to see the impact on the inversion resuts but fix the amount of assumed error in **S<sub>o<sub>** to simulate real-world violations of that assumption.    \n",
    "\n",
    "### Example 3:\n",
    "### Changing prior and covariance parameters on **S<sub>o** to explore how prior information and uncertainty assumptions impact the inversion results.  The point of this example is to help illustrate how bias, structure, and scale in the prior and **S<sub>o** influences the model's ability to recover the truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13af832-b8c7-464f-9791-19f52cea7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%Choosing your group's \n",
    "Exercise1 = False\n",
    "Exercise2 = False\n",
    "Exercise3 = True\n",
    "\n",
    "bias = 0\n",
    "#variables for the inversion DO NOT CHANGE\n",
    "priorslist = ['ACES_FFDAS', 'GRAAPESCO2']\n",
    "tower_names = ['NEB','NWB','HAL']\n",
    "monthlist = ['02','07']\n",
    "truemet = 'WRF2' \n",
    "met = truemet \n",
    "unc = False\n",
    "z_directory = 'G:/SummerSchool/output/enhancements/'\n",
    "y_save_directory = 'G:/SummerSchool/output/y/'\n",
    "\n",
    "if Exercise1 or Exercise2:  #R = Sz, y = z, Q = So\n",
    "    truth = 'ACES_FFDAS'  \n",
    "    prior = 'ACES_FFDAS'\n",
    "    y_bias = 0\n",
    "    R_ones = True\n",
    "    qones = True #Covariance matrix So is a diagonal of ones \n",
    "    q_param = 1 #This is a scaling factor to multiply So. Keep (1) for Exercise 1& 2, Change for Excercise 3 (b) and (c)-----> CHANGE THIS\n",
    "    q_param = q_param**2\n",
    "    R_whitenoise_mean_feb = 0\n",
    "    R_whitenoise_std_feb = 0\n",
    "    R_whitenoise_mean_jul = 0\n",
    "    R_whitenoise_std_jul = 0\n",
    "elif Exercise3:\n",
    "    y_bias = 0\n",
    "    R_ones = True\n",
    "    R_whitenoise_mean_feb = 0\n",
    "    R_whitenoise_std_feb = 0\n",
    "    R_whitenoise_mean_jul = 0\n",
    "    R_whitenoise_std_jul = 0\n",
    "    qones = False # (1) = TRUE, (2) & (3) = FALSE ----->  CHANGE THIS\n",
    "\n",
    "if Exercise1: #R = Sz, y = z\n",
    "    R_param_feb = 0.05 #This is a scaling factor that you use to multiply on your Sz matrix\n",
    "    R_param_feb =  R_param_feb**2 #to ppm2\n",
    "    R_param_jul = 0.05 #This is a scaling factor that you use to multiply on your Sz matrix\n",
    "    R_param_jul =  R_param_jul**2 #to ppm2\n",
    "    y_whitenoise_mean_feb = 0 #Do not change this as we want mean 0\n",
    "    y_whitenoise_std_feb = 0 #No noise\n",
    "    y_whitenoise_mean_jul = 0 #Do not change this as we want mean 0\n",
    "    y_whitenoise_std_jul = 0 #No noise\n",
    "\n",
    "if Exercise2: #R = So, y = z\n",
    "    R_param_feb = 0.1 #This is a scaling factor that you use to multiply on your Sz matrix\n",
    "    R_param_feb =  R_param_feb**2 #to ppm2\n",
    "    R_param_jul = 0.1 #This is a scaling factor that you use to multiply on your Sz matrix\n",
    "    R_param_jul =  R_param_jul**2 #to ppm2\n",
    "    y_whitenoise_mean_feb = 0 #Do not change this as we want mean 0\n",
    "    y_whitenoise_std_feb = 0.1 #Picarro = 0.1, K30 = 30, very low cost sensor = 50 ----->  CHANGE THIS\n",
    "    y_whitenoise_mean_jul = 0 #Do not change this as we want mean 0\n",
    "    y_whitenoise_std_jul = 0.1 #Picarro = 0.1, K30 = 30, very low cost sensor = 50 ----->  CHANGE THIS\n",
    "\n",
    "if Exercise3: \n",
    "    #IMPACT OF So & Sz\n",
    "    qones = False #(1) True (2) & (3) are False\n",
    "    truth = 'ACES_FFDAS'  \n",
    "    prior = 'ACES_FFDAS' #(1) 'GRAAPESCO2', (2) 'ACES_FFDAS', & (3) 'ACES_FFDAS'  ----->  CHANGE THIS\n",
    "    if qones:  #-------> CHANGE THIS! Make sure that it is (1) TRUE and (2) & (3) FALSE\n",
    "        q_param_feb = 1 #(1) (Tight) \n",
    "        q_param_jul = 1 #(1) (Tight)  \n",
    "        q_param_feb = q_param_feb**2\n",
    "        q_param_jul = q_param_jul**2 \n",
    "        R_param_feb = 0.1 #(1) = 0.1  \n",
    "        R_param_feb =  R_param_feb**2 #to ppm2\n",
    "        R_param_jul = 0.1 #(1) & (2) = 0.05 Very Small\n",
    "        R_param_jul =  R_param_jul**2 #to ppm2 \n",
    "        prior_multiplier = 1 #Do not change\n",
    "    else: #Diag of R is (1) prior^2 or (2) (0.5*truth)^2\n",
    "        q_param_feb = 10  #(2) = 1 (tight), & (3) = 10 (Really Loose) ----->  CHANGE THIS\n",
    "        q_param_jul = 10 #(2) = 1 (tight), & (3) = 10 (Really Loose) ----->  CHANGE THIS\n",
    "        q_floor = 1\n",
    "        prior_multiplier = 0.75 #(1) & (2) = 1, (3) = 0.75 ----->  CHANGE THIS\n",
    "        # Keep Picarro like noise on z but account for it in Sz\n",
    "        R_param_feb = 0.05 #(2) = 0.05 (tighgly to obs) (3) 0.05 (tighgly to obs) \n",
    "        R_param_feb =  R_param_feb**2 #to ppm2\n",
    "        R_param_jul = 0.05 #(2) = 0.05 (tighgly to obs) (3) 0.05 (tighgly to obs) \n",
    "        R_param_jul =  R_param_jul**2 #to ppm2 \n",
    "    y_whitenoise_mean_feb = 0 #Do not change this as we want mean 0; but you can change this later to explore constant bias\n",
    "    y_whitenoise_std_feb = 0.1 #Picarro = 0.1, K30 = 30, very low cost sensor = 50 \n",
    "    y_whitenoise_mean_jul = 0 #Do not change this as we want mean 0; but you can change this later to explore constant bias\n",
    "    y_whitenoise_std_jul = 0.1 #Picarro = 0.1, K30 = 30, very low cost sensor = 50\n",
    "\n",
    "print(' ')\n",
    "print('Met/Dispersion model = ' + truemet + '-STILT')\n",
    "print('True emissions = ' + truth)\n",
    "print('Prior fluxes = '+ prior) \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c6bf2-2bf0-4b2b-b0da-907738937237",
   "metadata": {},
   "source": [
    "#### In the next block, we are loading data, adding noise (if specified), and computing the mean enhancement of **ztruth**\n",
    "#### Write down answers on your sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ed06c-e720-4190-a252-11eff80e7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Data ...')\n",
    "def load_numpy_array(directory,fname):\n",
    "    filename = f'{directory}{fname}'\n",
    "    data = np.load(filename)\n",
    "    return data\n",
    "\n",
    "#load\n",
    "values_feb_truth = []\n",
    "values_feb_prior = []\n",
    "values_jul_truth = []\n",
    "values_jul_prior = []\n",
    "unit_WRF2_feb =[]\n",
    "unit_WRF2_jul = []\n",
    "\n",
    "for site in tower_names:\n",
    "    #Feb\n",
    "    filename_feb_prior = site + '_y_' + prior+'_02_2019.npy'\n",
    "    feb_values_prior = load_numpy_array(y_save_directory,filename_feb_prior)\n",
    "    values_feb_prior.append(feb_values_prior)\n",
    "    \n",
    "    filename_feb_truth = site + '_y_' + truth+'_02_2019.npy'\n",
    "    feb_values_truth = load_numpy_array(y_save_directory,filename_feb_truth)\n",
    "    values_feb_truth.append(feb_values_truth)\n",
    "    #print('y (feb)'+site + ' ' + str(len(feb_values_truth)))\n",
    "\n",
    "    u_WRF2_feb = np.load(z_directory + 'unit_'+ site+'_2019_02_WRF2.npy') #change later\n",
    "    u_WRF2_feb = [x for x in u_WRF2_feb if not pd.isna(x) and x!='nan']\n",
    "    u_WRF2_feb  = np.array(u_WRF2_feb)\n",
    "    unit_WRF2_feb.append(u_WRF2_feb) \n",
    "    \n",
    "    #Jul    \n",
    "    filename_jul_prior = site + '_y_' + prior+'_07_2019.npy'\n",
    "    jul_values_prior = load_numpy_array(y_save_directory,filename_jul_prior)\n",
    "    values_jul_prior.append(jul_values_prior)\n",
    "    \n",
    "    filename_jul_truth = site + '_y_' + truth+'_07_2019.npy'\n",
    "    jul_values_truth = load_numpy_array(y_save_directory,filename_jul_truth)\n",
    "    values_jul_truth.append(jul_values_truth)\n",
    "    \n",
    "    u_WRF2_jul = np.load(z_directory + 'unit_'+ site+'_2019_07_WRF2.npy') #change later\n",
    "    u_WRF2_jul = [x for x in u_WRF2_jul if not pd.isna(x) and x!='nan']\n",
    "    u_WRF2_jul  = np.array(u_WRF2_jul)\n",
    "    unit_WRF2_jul.append(u_WRF2_jul)\n",
    "\n",
    "#Feb\n",
    "y_feb_array_truth = np.concatenate(values_feb_truth)\n",
    "y_feb_array_prior = np.concatenate(values_feb_prior)\n",
    "r_unit_WRF2_feb = np.concatenate(unit_WRF2_feb)\n",
    "feb_mean = np.mean(y_feb_array_truth)\n",
    "feb_mean_pr = np.mean(y_feb_array_prior)\n",
    "feb_mean_u = np.mean(r_unit_WRF2_feb)\n",
    "\n",
    "#Jul\n",
    "y_jul_array_truth = np.concatenate(values_jul_truth)\n",
    "y_jul_array_prior = np.concatenate(values_jul_prior)\n",
    "r_unit_WRF2_jul = np.concatenate(unit_WRF2_jul)\n",
    "jul_mean = np.mean(y_jul_array_truth)\n",
    "jul_mean_pr = np.mean(y_jul_array_prior)\n",
    "jul_mean_u = np.mean(r_unit_WRF2_jul)\n",
    "\n",
    "noise_feb = np.random.normal(y_whitenoise_mean_feb, y_whitenoise_std_feb, size = y_feb_array_prior.shape)\n",
    "noise_jul = np.random.normal(y_whitenoise_mean_jul, y_whitenoise_std_jul, size = y_jul_array_prior.shape)\n",
    "\n",
    "y_feb_array_truth_hold = y_feb_array_truth.copy()\n",
    "y_jul_array_truth_hold = y_jul_array_truth.copy()\n",
    "\n",
    "y_feb_array_truth = y_feb_array_truth+noise_feb+bias\n",
    "y_jul_array_truth = y_jul_array_truth+noise_jul+bias    \n",
    "\n",
    "print(' ')\n",
    "print('Mean enhancement (ztruth) for Feb is '+ str(round(feb_mean,2)) +' ppm')\n",
    "print('Mean enhancement (ztruth) for Jul is '+ str(round(jul_mean,2)) + ' ppm')\n",
    "print(' ')\n",
    "\n",
    "print('Mean prior enhancement for Feb is '+ str(round(feb_mean_pr,2)) +' ppm')\n",
    "print('Mean prior enhancement for Jul is '+ str(round(jul_mean_pr,2)) + ' ppm')\n",
    "print('For exercise 3d - skip printing these enhancements')\n",
    "print(' ')\n",
    "    \n",
    "print('Mean unit enhancement for Feb is '+ str(round(feb_mean_u,2)) +' ppm')\n",
    "print('Mean unit enhancement for Jul is '+ str(round(jul_mean_u,2)) + ' ppm')\n",
    "\n",
    "print(' ')\n",
    "print('done!')\n",
    "print('Configuring Sz')\n",
    "print(' ')\n",
    "\n",
    "#Create Sz\n",
    "if R_ones:\n",
    "    R_Feb = np.ones(y_feb_array_prior.shape)\n",
    "    R_Jul = np.ones(y_jul_array_prior.shape)\n",
    "    noise_feb = np.random.normal(R_whitenoise_mean_feb,R_whitenoise_std_feb, size = y_feb_array_prior.shape)\n",
    "    noise_jul = np.random.normal(R_whitenoise_mean_jul,R_whitenoise_std_jul, size = y_jul_array_prior.shape)\n",
    "    R_Feb = (R_Feb+noise_feb)*R_param_feb\n",
    "    R_Jul = (R_Jul+noise_jul)*R_param_jul\n",
    "    print('Sz = diag of one for Feb and July')\n",
    "else:\n",
    "    R_Feb  = u_WRF2_feb*R_param_feb\n",
    "    R_Jul  = u_WRF2_jul*R_param_jul\n",
    "    print('Sz = diag of unit enhancements for Feb and July')\n",
    "\n",
    "print(' ')\n",
    "print('Scaling factor on Sz is ' + str(np.sqrt(R_param_feb)) + ' ppm in Feb')\n",
    "print('Scaling factor on Sz is ' + str(np.sqrt(R_param_jul)) + ' ppm in Jul')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099eb22-0ea9-4427-86c7-a8461a95a005",
   "metadata": {},
   "source": [
    "#### What what do these average hourly enhancements mean in terms of the values of the errors you specify in **S<sub>z**?  \n",
    "\n",
    "##### In the next block, we are configuring **S<sub>z<sub>** data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f457d0-c0c4-489f-bc10-14e38a858caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Creating So & Loading Hmatrix\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.sparse import diags  # still used for diagonal case\n",
    "\n",
    "use_spatial_correlation = True\n",
    "\n",
    "def load_sparse_matrix(filename):\n",
    "    return sp.load_npz(filename)\n",
    "\n",
    "Hmatrix_feb = load_sparse_matrix('G:/SummerSchool/output/Hmatrices/H_'+met+'_2019_02.npz') \n",
    "Hmatrix_jul = load_sparse_matrix('G:/SummerSchool/output/Hmatrices/H_'+met+'_2019_07.npz') \n",
    "\n",
    "prior_array_feb = np.load('G:/SummerSchool/output/prior/'+prior+'_2019_02.npy')*prior_multiplier\n",
    "prior_array_jul = np.load('G:/SummerSchool/output/prior/'+prior+'_2019_07.npy')*prior_multiplier\n",
    "\n",
    "truth_array_feb = np.load('G:/SummerSchool/output/prior/'+truth+'_2019_02.npy')\n",
    "truth_array_jul = np.load('G:/SummerSchool/output/prior/'+truth+'_2019_07.npy')\n",
    "q_size_jul = len(prior_array_jul)\n",
    "q_size_feb = len(prior_array_feb)\n",
    "\n",
    "#Check dimensions\n",
    "print('')\n",
    "print(f\"Hmatrix shape(Feb): {Hmatrix_feb.shape}\")\n",
    "print('Prior length: ' + str(len(prior_array_feb)))\n",
    "print('Size of So (Feb) = ' + str(q_size_feb) + ' x ' +str(q_size_feb))\n",
    "print(' ')\n",
    "print('Mean prior emission value for Feb is '+ str(round(np.mean(prior_array_feb),2)) + ' umol/m2s')\n",
    "percentile_75_feb = np.percentile(prior_array_feb, 75)\n",
    "print('75th percentile of prior emission value for Feb is '+ str(round(percentile_75_feb,2)) + ' umol/m2s')\n",
    "print(' ')\n",
    "print('Mean truth emission value for Feb is '+ str(round(np.mean(truth_array_feb),2)) + ' umol/m2s')\n",
    "print(' ')\n",
    "print(f\"Hmatrix shape(July): {Hmatrix_jul.shape}\")    \n",
    "print('Size of So (Jul) = ' + str(q_size_jul) + ' x ' +str(q_size_jul))\n",
    "print('Prior length: ' + str(len(prior_array_jul)))\n",
    "print(' ')\n",
    "print('Mean prior emission value for Jul is '+ str(round(np.mean(prior_array_jul),2)) + ' umol/m2s')\n",
    "percentile_75_jul = np.percentile(prior_array_jul, 75)\n",
    "print('75th percentile of prior emission value for Jul is '+ str(round(percentile_75_jul,2)) + ' umol/m2s')\n",
    "print(' ')\n",
    "print('Mean truth emission value for Jul is '+ str(round(np.mean(truth_array_jul),2)) + ' umol/m2s')\n",
    "\n",
    "Hsp_jul = Hmatrix_jul@prior_array_jul #should be the same as original y_jul_array_prior\n",
    "Hsp_feb = Hmatrix_feb@prior_array_feb #should be the same as original y_feb_array_prior\n",
    "\n",
    "print(' ')\n",
    "print('Created Hxsp')\n",
    "print(' ')\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(18,6))\n",
    "ax[0].plot(y_feb_array_truth,label='ztrue enh. w noise and bias',color='black',linewidth =.75)\n",
    "ax[0].plot(Hsp_feb,label='Hxprior:modelled enhan.',color='red',linewidth =1)\n",
    "ax[0].legend(fontsize=14)  # Adjust font size for legend entries\n",
    "ax[0].set_ylabel('ppm', fontsize=14)  # Adjust font size for y-axis label\n",
    "ax[0].set_xlabel('Time index (hourly) w gaps', fontsize=14)  # Adjust font size for x-axis label\n",
    "ax[0].set_title(\"ztruth vs Hxprior Feb 2019\", fontsize=14) \n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(y_jul_array_truth,label='ztrue enh. w noise and bias',color='black',linewidth =.75)\n",
    "ax[1].plot(Hsp_jul,label='Hxprior:modelled enh.',color='red',linewidth =1)\n",
    "ax[1].legend(fontsize=14)  # Adjust font size for legend entries\n",
    "ax[1].set_ylabel('ppm', fontsize=14)  # Adjust font size for y-axis label\n",
    "ax[1].set_xlabel('Time index (hourly) w gaps', fontsize=14)  # Adjust font size for x-axis label\n",
    "ax[1].set_title(\"ztruth vs Hxprior Jul 2019\", fontsize=14)\n",
    "ax[1].grid(True)\n",
    "\n",
    "print('Mean of difference between Hxprior and ztrue (Feb) = ' + str(round(np.mean(Hsp_feb - y_feb_array_truth),4)))\n",
    "print('Mean of difference between Hxprior and ztrue (Jul) = ' + str(round(np.mean(Hsp_jul - y_jul_array_truth),4)))\n",
    "print('')\n",
    "if qones:\n",
    "    print('So is diag of ones')\n",
    "    Q_diag_feb = (np.ones(prior_array_feb.shape[0])*q_param_feb)\n",
    "    q_feb = Q_diag_feb\n",
    "    Q_diag_feb = diags(Q_diag_feb,0)\n",
    "    Q_diag_jul = (np.ones(prior_array_jul.shape[0])*q_param_jul)\n",
    "    q_jul = Q_diag_jul\n",
    "    Q_diag_jul = diags(Q_diag_jul,0)\n",
    "else:\n",
    "    print('So is varying (per prior values)') \n",
    "    q_prior_array_feb = prior_array_feb.copy()\n",
    "    q_prior_array_feb[q_prior_array_feb < 1] = 1 #give So a floor of one\n",
    "    q_diag_feb = (q_param_feb) * np.square(q_prior_array_feb)\n",
    "    q_feb = q_diag_feb\n",
    "    Q_diag_feb = diags(q_diag_feb,0)\n",
    "    q_prior_array_jul =prior_array_jul.copy()\n",
    "    q_prior_array_jul[q_prior_array_jul < 1] = q_floor\n",
    "    q_diag_jul = (q_param_jul) * np.square(q_prior_array_jul)\n",
    "    Q_diag_jul = diags(q_diag_jul,0)\n",
    "    q_jul = q_diag_jul\n",
    "\n",
    "print('') \n",
    "print('Scaling factor on Sz is ' + str(round(np.sqrt(q_param_feb),3)) + ' ppm in Feb')\n",
    "print('Scaling factor on Sz is ' + str(round(np.sqrt(q_param_feb),3)) + ' ppm in Jul')\n",
    "print(' ')\n",
    "print('Size of So diag (Feb) = ' +  str(q_feb.shape[0]))\n",
    "print('Size of So diag (Jul) = ' +  str(q_jul.shape[0]))\n",
    "\n",
    "print('')\n",
    "print('Created So July and So Feb')\n",
    "print(' ')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a365a-dc39-4966-8462-f4c645cecff8",
   "metadata": {},
   "source": [
    "#### What is the relationship between the ztrue and the red line?\n",
    "\n",
    "##### In this block, we create **S<sub>o</sub>** and load our **H** matrices for February and July 2019. You always want to check your dimensions on everything.  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d243fa-6b63-43b1-a05a-08dfa8c42be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Creating So & Loading Hmatrix\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.sparse import diags  # still used for diagonal case\n",
    "\n",
    "use_spatial_correlation = True\n",
    "\n",
    "def load_sparse_matrix(filename):\n",
    "    return sp.load_npz(filename)\n",
    "\n",
    "Hmatrix_feb = load_sparse_matrix('G:/SummerSchool/output/Hmatrices/H_'+met+'_2019_02.npz') \n",
    "Hmatrix_jul = load_sparse_matrix('G:/SummerSchool/output/Hmatrices/H_'+met+'_2019_07.npz') \n",
    "\n",
    "prior_array_feb = np.load('G:/SummerSchool/output/prior/'+prior+'_2019_02.npy')*prior_multiplier\n",
    "prior_array_jul = np.load('G:/SummerSchool/output/prior/'+prior+'_2019_07.npy')*prior_multiplier\n",
    "\n",
    "truth_array_feb = np.load('G:/SummerSchool/output/prior/'+truth+'_2019_02.npy')\n",
    "truth_array_jul = np.load('G:/SummerSchool/output/prior/'+truth+'_2019_07.npy')\n",
    "q_size_jul = len(prior_array_jul)\n",
    "q_size_feb = len(prior_array_feb)\n",
    "\n",
    "#Check dimensions\n",
    "print('')\n",
    "print(f\"Hmatrix shape(Feb): {Hmatrix_feb.shape}\")\n",
    "print('Prior length: ' + str(len(prior_array_feb)))\n",
    "print('Size of So (Feb) = ' + str(q_size_feb) + ' x ' +str(q_size_feb))\n",
    "print(' ')\n",
    "print('Mean prior emission value for Feb is '+ str(round(np.mean(prior_array_feb),2)) + ' umol/m2s')\n",
    "percentile_75_feb = np.percentile(prior_array_feb, 75)\n",
    "print('75th percentile of prior emission value for Feb is '+ str(round(percentile_75_feb,2)) + ' umol/m2s')\n",
    "print(' ')\n",
    "print('Mean truth emission value for Feb is '+ str(round(np.mean(truth_array_feb),2)) + ' umol/m2s')\n",
    "print(' ')\n",
    "print(f\"Hmatrix shape(July): {Hmatrix_jul.shape}\")    \n",
    "print('Size of So (Jul) = ' + str(q_size_jul) + ' x ' +str(q_size_jul))\n",
    "print('Prior length: ' + str(len(prior_array_jul)))\n",
    "print(' ')\n",
    "print('Mean prior emission value for Jul is '+ str(round(np.mean(prior_array_jul),2)) + ' umol/m2s')\n",
    "percentile_75_jul = np.percentile(prior_array_jul, 75)\n",
    "print('75th percentile of prior emission value for Jul is '+ str(round(percentile_75_jul,2)) + ' umol/m2s')\n",
    "print(' ')\n",
    "print('Mean truth emission value for Jul is '+ str(round(np.mean(truth_array_jul),2)) + ' umol/m2s')\n",
    "\n",
    "Hsp_jul = Hmatrix_jul@prior_array_jul #should be the same as original y_jul_array_prior\n",
    "Hsp_feb = Hmatrix_feb@prior_array_feb #should be the same as original y_feb_array_prior\n",
    "\n",
    "print(' ')\n",
    "print('Created Hxsp')\n",
    "print(' ')\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(18,6))\n",
    "ax[0].plot(y_feb_array_truth,label='ztrue enh. w noise and bias',color='black',linewidth =.75)\n",
    "ax[0].plot(Hsp_feb,label='Hxprior:modelled enhan.',color='red',linewidth =1)\n",
    "ax[0].legend(fontsize=14)  # Adjust font size for legend entries\n",
    "ax[0].set_ylabel('ppm', fontsize=14)  # Adjust font size for y-axis label\n",
    "ax[0].set_xlabel('Time index (hourly) w gaps', fontsize=14)  # Adjust font size for x-axis label\n",
    "ax[0].set_title(\"ztruth vs Hxprior Feb 2019\", fontsize=14) \n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(y_jul_array_truth,label='ztrue enh. w noise and bias',color='black',linewidth =.75)\n",
    "ax[1].plot(Hsp_jul,label='Hxprior:modelled enh.',color='red',linewidth =1)\n",
    "ax[1].legend(fontsize=14)  # Adjust font size for legend entries\n",
    "ax[1].set_ylabel('ppm', fontsize=14)  # Adjust font size for y-axis label\n",
    "ax[1].set_xlabel('Time index (hourly) w gaps', fontsize=14)  # Adjust font size for x-axis label\n",
    "ax[1].set_title(\"ztruth vs Hxprior Jul 2019\", fontsize=14)\n",
    "ax[1].grid(True)\n",
    "\n",
    "print('Mean of difference between Hxprior and ztrue (Feb) = ' + str(round(np.mean(Hsp_feb - y_feb_array_truth),4)))\n",
    "print('Mean of difference between Hxprior and ztrue (Jul) = ' + str(round(np.mean(Hsp_jul - y_jul_array_truth),4)))\n",
    "print('')\n",
    "if qones:\n",
    "    print('So is diag of ones')\n",
    "    Q_diag_feb = (np.ones(prior_array_feb.shape[0])*q_param_feb)\n",
    "    q_feb = Q_diag_feb\n",
    "    Q_diag_feb = diags(Q_diag_feb,0)\n",
    "    Q_diag_jul = (np.ones(prior_array_jul.shape[0])*q_param_jul)\n",
    "    q_jul = Q_diag_jul\n",
    "    Q_diag_jul = diags(Q_diag_jul,0)\n",
    "else:\n",
    "    print('So is varying (per prior values)') \n",
    "    q_prior_array_feb = prior_array_feb.copy()\n",
    "    q_prior_array_feb[q_prior_array_feb < 1] = 1 #give So a floor of one\n",
    "    q_diag_feb = (q_param_feb) * np.square(q_prior_array_feb)\n",
    "    q_feb = q_diag_feb\n",
    "    Q_diag_feb = diags(q_diag_feb,0)\n",
    "    q_prior_array_jul =prior_array_jul.copy()\n",
    "    q_prior_array_jul[q_prior_array_jul < 1] = q_floor\n",
    "    q_diag_jul = (q_param_jul) * np.square(q_prior_array_jul)\n",
    "    Q_diag_jul = diags(q_diag_jul,0)\n",
    "    q_jul = q_diag_jul\n",
    "\n",
    "print('') \n",
    "print('Scaling factor on Sz is ' + str(round(np.sqrt(q_param_feb),3)) + ' ppm in Feb')\n",
    "print('Scaling factor on Sz is ' + str(round(np.sqrt(q_param_feb),3)) + ' ppm in Jul')\n",
    "print(' ')\n",
    "print('Size of So diag (Feb) = ' +  str(q_feb.shape[0]))\n",
    "print('Size of So diag (Jul) = ' +  str(q_jul.shape[0]))\n",
    "\n",
    "print('')\n",
    "print('Created So July and So Feb')\n",
    "print(' ')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93404855-2441-4132-ad26-549854ffbbd5",
   "metadata": {},
   "source": [
    "#### What can you say about how your modelled enhancements, i.e. **Hx<sub>prior<sub>**, are different than your **ztruth**?   \n",
    "\n",
    "#### How does that compare with your true enhancement?  \n",
    "\n",
    " \n",
    "##### In the next block, we will create all the pieces for the inversion for Feb and July 2019 --- and most importantly, estimate our emissions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7a614-a263-4885-94cd-328cfcf9a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HSoHt &SoHt Feb and July\n",
    "HQ_feb = Hmatrix_feb@Q_diag_feb\n",
    "HQ_jul = Hmatrix_jul@Q_diag_jul\n",
    "\n",
    "def create_diagonal_matrix(vector):\n",
    "  vector_size = len(vector)\n",
    "  diagonal_matrix = np.zeros((vector_size, vector_size))\n",
    "  diagonal_matrix[np.diag_indices(vector_size)] = vector\n",
    "  return diagonal_matrix\n",
    "\n",
    "R_diagonal_Feb = create_diagonal_matrix(R_Feb)\n",
    "R_diagonal_Jul = create_diagonal_matrix(R_Jul)\n",
    "\n",
    "Htrans_feb = Hmatrix_feb.T\n",
    "HQHt_feb = HQ_feb@Htrans_feb\n",
    "QHt_feb = Q_diag_feb@Htrans_feb\n",
    "Psi_feb = HQHt_feb.toarray()+R_Feb\n",
    "Psi_inv_feb = dense_inv(Psi_feb)\n",
    "\n",
    "Htrans_jul = Hmatrix_jul.T\n",
    "HQHt_jul = HQ_jul@Htrans_jul\n",
    "QHt_jul = Q_diag_jul@Htrans_jul\n",
    "HQ_jul = Hmatrix_jul@Q_diag_jul\n",
    "Psi_jul = HQHt_jul.toarray()+R_Jul\n",
    "Psi_inv_jul = dense_inv(Psi_jul)\n",
    "\n",
    "psi_z_feb= Psi_inv_feb@(y_feb_array_truth-Hsp_feb)\n",
    "shat_feb = prior_array_feb+QHt_feb@psi_z_feb \n",
    "\n",
    "psi_z_jul= Psi_inv_jul@(y_jul_array_truth-Hsp_jul)\n",
    "shat_jul = prior_array_jul+QHt_jul@psi_z_jul \n",
    "\n",
    "print('')\n",
    "print('Feb: HSo (' +  str(HQ_feb.shape[0]) + ',' + str(HQ_feb.shape[1]) +')')\n",
    "print('July: HSo (' +  str(HQ_jul.shape[0]) + ',' + str(HQ_jul.shape[1]) +')')\n",
    "print(' ')\n",
    "print('Feb: Sz diagonal (' +  str(R_diagonal_Feb.shape[0]) + ',' +  str(R_diagonal_Feb.shape[1]) +')')\n",
    "print('July: Sz diagonal (' + str(R_diagonal_Jul.shape[0]) + ',' +  str(R_diagonal_Jul.shape[1]) +')')\n",
    "print(' ')\n",
    "print('Created HSoHt +Sz (Psi) and inv(Psi) for Feb and July')\n",
    "print('')\n",
    "print('Feb and Jul emissions shat estimates are completed')\n",
    "print(' ')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8fcdf-c0ec-419c-9bd4-444fb3196828",
   "metadata": {},
   "source": [
    "##### In the next block, (if you want to) we will \"approximate\" uncertainty associated with the **x<sub>hat<sub>** emissions in Feb and July 2019.  This takes too long for our excercise time so we are going to skip it.  \n",
    "\n",
    "We cannot take the inverse of So so we have to do some slicing for an approximation.  Do not use this code outside of this toy exercise for your own research.  Use the equations provided in the batch inversion day to estimate uncertainties - but use a computer with a lot of memory or use methods specified in Yadav et al. ()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115ceed-25a6-434b-8e73-6aed120f5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimating Uncertainties\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "if unc:\n",
    "    def compute_posterior_uncertainty_sparse(H, Q_diag, R_diag):\n",
    "        \"\"\"\n",
    "        Efficiently compute the diagonal of the posterior covariance matrix.\n",
    "\n",
    "        Parameters:\n",
    "            H:       csr_matrix, shape (m, n), sensitivity matrix\n",
    "            Q_diag:  1D array or diagonal matrix, shape (n,)\n",
    "            R_diag:  1D array or diagonal matrix, shape (m,)\n",
    "\n",
    "        Returns:\n",
    "            posterior_std: 1D array of posterior standard deviations\n",
    "            avg_std: float, mean of posterior std devs\n",
    "        \"\"\"\n",
    "        if not isinstance(H, csr_matrix):\n",
    "            H = H.tocsr()\n",
    "\n",
    "        m, n = H.shape\n",
    "        q_diag = Q_diag.diagonal() if hasattr(Q_diag, \"diagonal\") else Q_diag\n",
    "        r_diag = R_diag.diagonal() if hasattr(R_diag, \"diagonal\") else R_diag\n",
    "        R_inv_diag = 1.0 / r_diag\n",
    "\n",
    "        vshat_diag = np.copy(q_diag)\n",
    "\n",
    "        print(\"Computing diagonal of posterior covariance...\")\n",
    "\n",
    "        for i in tqdm(range(n), desc=\"State variable\"):\n",
    "            h_col = H[:, i].toarray().flatten()  # shape (m,)\n",
    "            if np.any(h_col):  # skip if all zero\n",
    "                vshat_diag[i] -= q_diag[i]**2 * np.sum((h_col**2) * R_inv_diag)\n",
    "\n",
    "        posterior_std = np.sqrt(np.maximum(vshat_diag, 0))\n",
    "        return posterior_std, np.mean(posterior_std)\n",
    "\n",
    "    posterior_std_feb, avg_std_feb = compute_posterior_uncertainty_sparse(Hmatrix_feb, Q_diag_feb, R_diagonal_Feb)\n",
    "    posterior_std_jul, avg_std_jul = compute_posterior_uncertainty_sparse(Hmatrix_jul, Q_diag_jul, R_diagonal_Jul)\n",
    "\n",
    "    print(f\"Average posterior std (Feb): {avg_std_feb:.4f} µmol/m²/s\")\n",
    "    print(f\"Average posterior std (Jul): {avg_std_jul:.4f} µmol/m²/s\")\n",
    "    vshat_diag_feb = posterior_std_feb\n",
    "    vshat_diag_jul = posterior_std_jul\n",
    "\n",
    "else: \n",
    "    print('')\n",
    "    print('Not runnuning uncertainties so posterior uncertaites equal prior posterior uncertainties.')\n",
    "    vshat_diag_feb = q_feb \n",
    "    vshat_diag_jul = q_jul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2317bff-a361-4ba0-b03c-80c45b3919e2",
   "metadata": {},
   "source": [
    "##### Next code block is **UGLY** code.  But the code is rearraging things to look at the monthly mean.  (Not part of this exercise) You can change code to see how this looks at other time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4d99a-8744-44f1-8762-eeec91fb7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#Rearranging shat, priors, and truths\n",
    "utctime_feb = np.load('G:/SummerSchool/output/prior/H_ACES_FFDAS_2019_02_utctimes.npy',allow_pickle=True)\n",
    "utctime_jul = np.load('G:/SummerSchool/output/prior/H_ACES_FFDAS_2019_07_utctimes.npy',allow_pickle=True)\n",
    "nhrs_back = 180\n",
    "#February\n",
    "#Shat\n",
    "reshape_shat_feb = shat_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "trimmed_shat_feb = reshape_shat_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_shat_feb = trimmed_shat_feb.ravel(order='F')\n",
    "mean_shat_feb = np.mean(trimmed_shat_feb,axis = 0)\n",
    "mean_shat_feb_week = np.mean(trimmed_shat_feb, axis=1)\n",
    "#Posterior Unc Vshat\n",
    "reshape_vshat_diag_feb = vshat_diag_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "trimmed_vshat_diag_feb = reshape_vshat_diag_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_vshat_diag_feb = trimmed_vshat_diag_feb.ravel(order='F')\n",
    "mean_vshat_diag_feb = np.mean(trimmed_vshat_diag_feb,axis = 0)\n",
    "print('')\n",
    "#if unc:\n",
    "#    print('average posterior uncertainty Feb = ' + str(round(np.sqrt(np.mean(mean_vshat_diag_feb)),4)))\n",
    "    \n",
    "#Prior Unc Q\n",
    "reshape_q_diag_feb = q_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "trimmed_q_diag_feb = reshape_q_diag_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_q_diag_feb = trimmed_q_diag_feb.ravel(order='F')\n",
    "mean_q_diag_feb = np.mean(trimmed_q_diag_feb,axis = 0)\n",
    "#print('average prior uncertainty Feb = ' + str(round(np.sqrt(np.mean(mean_q_diag_feb)),4)))\n",
    "\n",
    "#July\n",
    "#Shat\n",
    "reshape_shat_jul = shat_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "trimmed_shat_jul = reshape_shat_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_shat_jul = trimmed_shat_jul.ravel(order='F')\n",
    "mean_shat_jul = np.mean(trimmed_shat_jul,axis = 0)\n",
    "mean_shat_jul_week = np.mean(trimmed_shat_jul, axis=1)\n",
    "#Posterior Unc Vshat\n",
    "reshape_vshat_diag_jul = vshat_diag_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "trimmed_vshat_diag_jul = reshape_vshat_diag_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_vshat_diag_jul = trimmed_vshat_diag_jul.ravel(order='F')\n",
    "mean_vshat_diag_jul = np.mean(trimmed_vshat_diag_jul,axis = 0)\n",
    "#if unc:\n",
    "#    print('average posterior uncertainty Jul = ' + str(round(np.sqrt(np.mean(mean_vshat_diag_jul)),4)))\n",
    "#Prior Unc Q\n",
    "reshape_q_diag_jul = q_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "trimmed_q_diag_jul = reshape_q_diag_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_q_diag_jul = trimmed_q_diag_jul.ravel(order='F')\n",
    "mean_q_diag_jul = np.mean(trimmed_q_diag_jul,axis = 0)\n",
    "#print('average prior uncertainty Jul = ' + str(round(np.sqrt(np.mean(mean_q_diag_jul)),4)))\n",
    "\n",
    "mean_shat_feb = mean_shat_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "mean_shat_jul = mean_shat_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "mean_vshat_diag_feb = mean_vshat_diag_feb.reshape(len(lon_grid),len(lat_grid)) #\n",
    "mean_vshat_diag_jul = mean_vshat_diag_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "prior_array_feb = prior_array_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "prior_array_feb = prior_array_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_prior_array_feb = prior_array_feb.ravel(order='F')\n",
    "mean_prior_array_feb = np.mean(prior_array_feb,axis = 0)\n",
    "mean_prior_array_feb = mean_prior_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "prior_array_jul = prior_array_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "prior_array_jul = prior_array_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_prior_array_jul = prior_array_jul.ravel(order='F')\n",
    "mean_prior_array_jul = np.mean(prior_array_jul,axis = 0)\n",
    "mean_prior_array_jul = mean_prior_array_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "q_diag_array_feb = q_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "q_diag_array_feb = q_diag_array_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_q_diag_array_feb = q_diag_array_feb.ravel(order='F')\n",
    "mean_q_diag_array_feb = np.mean(q_diag_array_feb,axis = 0)\n",
    "mean_q_diag_array_feb = mean_q_diag_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "q_diag_array_jul = q_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "q_diag_array_jul = q_diag_array_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_q_diag_array_jul = q_diag_array_jul.ravel(order='F')\n",
    "mean_q_diag_array_jul = np.mean(q_diag_array_jul,axis = 0)\n",
    "mean_q_diag_array_jul = mean_q_diag_array_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "mean_prior_array_feb = mean_prior_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "mean_prior_array_jul = mean_prior_array_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "mean_q_diag_array_feb = mean_q_diag_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon a\n",
    "mean_q_diag_array_jul = mean_q_diag_array_jul.reshape(len(lon_grid),len(lat_grid))\n",
    "\n",
    "#Truth Feb and July\n",
    "truth_array_feb = truth_array_feb.reshape(len(utctime_feb),num_fluxes)\n",
    "truth_array_feb = truth_array_feb[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_truth_array_feb = truth_array_feb.ravel(order='F')\n",
    "mean_truth_array_feb = np.mean(truth_array_feb,axis = 0)\n",
    "mean_truth_array_feb = mean_truth_array_feb.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "truth_array_jul = truth_array_jul.reshape(len(utctime_jul),num_fluxes)\n",
    "truth_array_jul = truth_array_jul[nhrs_back:-(nhrs_back-1),:]\n",
    "flatten_truth_array_jul = truth_array_jul.ravel(order='F')\n",
    "mean_truth_array_jul = np.mean(truth_array_jul,axis = 0)\n",
    "mean_truth_array_jul = mean_truth_array_jul.reshape(len(lon_grid),len(lat_grid)) #lon and lat may need to be flipped but don't know\n",
    "\n",
    "print('Configured xhat, xprior emissions, emissions truth, So!')\n",
    "if unc:\n",
    "    print('and Vxhat diagonal!')\n",
    "print(' ')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627cb4be-1f53-4af2-b642-a8acc33d1fe7",
   "metadata": {},
   "source": [
    "### For all exercises: \n",
    "#### What do the statistics for the full domain tell us?\n",
    "\n",
    "Note that the next code block **DOES NOT** calculate the chi-squared statistic.  This is something for you to do later.  It is an important metric to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392e810-537e-4d3f-9d9e-a6aba9be8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Calculating Statistics\n",
    "\n",
    "diff_prior_feb = mean_shat_feb-mean_prior_array_feb\n",
    "diff_prior_jul = mean_shat_jul-mean_prior_array_jul\n",
    "\n",
    "diff_truth_feb = mean_shat_feb-mean_truth_array_feb\n",
    "diff_truth_jul = mean_shat_jul-mean_truth_array_jul\n",
    "\n",
    "mean_diff_feb = round(np.mean(diff_truth_feb),4)\n",
    "mean_diff_jul = round(np.mean(diff_truth_jul),4)\n",
    "\n",
    "mean_prior_feb = round(np.mean(mean_prior_array_feb),4)\n",
    "mean_prior_jul = round(np.mean(mean_prior_array_jul),4)\n",
    "\n",
    "mean_truth_feb =round(np.mean(mean_truth_array_feb),4)\n",
    "mean_truth_jul =round(np.mean(mean_truth_array_jul),4)\n",
    "\n",
    "mean_shat_feb_val =round(np.mean(mean_shat_feb),4)\n",
    "mean_shat_jul_val =round(np.mean(mean_shat_jul),4)\n",
    "\n",
    "def calculate_rmse(truth, estimated):\n",
    "    assert truth.shape  == estimated.shape, \"Arrays must be the same shape!\"\n",
    "    rmse = np.sqrt(np.nanmean((truth-estimated)**2))\n",
    "    return rmse\n",
    "\n",
    "rmse_feb = calculate_rmse(flatten_shat_feb,flatten_truth_array_feb)\n",
    "rmse_jul = calculate_rmse(flatten_shat_jul,flatten_truth_array_jul)\n",
    "\n",
    "corr_matrix_feb = np.corrcoef(flatten_truth_array_feb,flatten_shat_feb)\n",
    "corr_coef_feb = corr_matrix_feb[0,1]\n",
    "corr_matrix_jul = np.corrcoef(flatten_truth_array_jul,flatten_shat_jul)\n",
    "corr_coef_jul = corr_matrix_jul[0,1]\n",
    "\n",
    "std_err_feb = np.std(flatten_shat_feb - flatten_truth_array_feb)/np.sqrt(len(flatten_truth_array_feb))\n",
    "std_err_jul = np.std(flatten_shat_jul - flatten_truth_array_jul)/np.sqrt(len(flatten_truth_array_jul))\n",
    "\n",
    "print('Statistics in flux space')\n",
    "print(' ')\n",
    "print('Mean truth flux (Feb):' + str(round(mean_truth_feb,3)) + ' µmol/m2s')\n",
    "print('Mean truth flux (Jul):' + str(round(mean_truth_jul,3)) + ' µmol/m2s')\n",
    "print(' ')\n",
    "print('Mean estimated flux (Feb):' + str(round(mean_shat_feb_val,3)) + ' µmol/m2s')\n",
    "print('Mean estimated flux (Jul):' + str(round(mean_shat_jul_val,3)) + ' µmol/m2s')\n",
    "print('')\n",
    "print('Mean prior flux (Feb):' + str(round(mean_prior_feb,3)) + ' µmol/m2s')\n",
    "print('Mean prior flux (Jul):' + str(round(mean_prior_jul,3)) + ' µmol/m2s')\n",
    "print('')\n",
    "print(f\"Mean difference (xhat - xtruth) Feb: {mean_diff_feb:.4f} µmol/m2s\")\n",
    "print(f\"Mean difference (xhat - xtruth) Jul: {mean_diff_jul:.4f} µmol/m2s\")\n",
    "print('')\n",
    "print(f\"Std Error Feb: {std_err_feb:.4f} µmol/m2s\")\n",
    "print(f\"Std Error Jul: {std_err_jul:.4f} µmol/m2s\")\n",
    "print(' ')\n",
    "print(f\"Correlation Coefficient (xhat,xtruth) Feb: {corr_coef_feb:.4f}\")\n",
    "print(f\"Correlation Coefficient (xhat,xtruth) Jul: {corr_coef_jul:.4f}\")\n",
    "print(' ')\n",
    "print(f\"RMSE Feb: {rmse_feb:.4f} µmol/m2s\")\n",
    "print(f\"RMSE Jul: {rmse_jul:.4f} µmol/m2s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e2914-7f1c-4a78-b409-3fea102b3a82",
   "metadata": {},
   "source": [
    "#### Let's check out how things look in space across our entire domain and look at differences.  \n",
    "- There will be 6 plots that show the estimates, priors, and truths.\n",
    "- IF we aren't running the perfect case, we will also have four more plots looking at differences.\n",
    "- We don't show the base case because there will be small spurious noise in the differences.\n",
    "\n",
    "##### **Note:** a lot of **UGLY** plotting code in this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e5b92-1ce7-4891-b520-c086805fe194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Plotting\n",
    "print('Plotting spatial maps')\n",
    "print('This can take awhile ... please be patient')\n",
    "ua_pathname = 'G:/NEC/Regional/shapefiles/Census/UrbanAreas/tl_2023_us_uac20.zip'\n",
    "gdf_ua = gpd.read_file(ua_pathname)\n",
    "gdf_ua.crs = \"EPSG:4326\"\n",
    "\n",
    "# Select Baltimore areas\n",
    "gdf_baltimore = gdf_ua[gdf_ua['NAME20'] == \"Baltimore, MD\"]\n",
    "ua_pathname = 'G:/NEC/Regional/shapefiles/Census/UrbanAreas/tl_2023_us_uac20.zip'\n",
    "gdf_ua = gpd.read_file(ua_pathname)\n",
    "gdf_ua.crs = \"EPSG:4326\"\n",
    "geometry = [-76.583, 39.315417] \n",
    "point = Point(geometry) \n",
    "twr_gdf_NEB = gpd.GeoDataFrame(crs=\"EPSG:4326\", geometry=[point])\n",
    "geometry = [-76.685071, 39.344541]  \n",
    "point = Point(geometry) \n",
    "twr_gdf_NWB = gpd.GeoDataFrame(crs=\"EPSG:4326\", geometry=[point])\n",
    "geometry = [-76.675278, 39.255194]  \n",
    "point = Point(geometry) \n",
    "twr_gdf_HAL = gpd.GeoDataFrame(crs=\"EPSG:4326\", geometry=[point])\n",
    "\n",
    "# Get bounds\n",
    "minx, miny, maxx, maxy = gdf_baltimore.total_bounds\n",
    "bounds = gdf_baltimore.total_bounds\n",
    "mask_polygon = box(*bounds)\n",
    "\n",
    "vmin_jul = np.percentile(flatten_shat_jul, 10)\n",
    "vmax_jul = np.percentile(flatten_shat_jul, 90)\n",
    "norm_jul = Normalize(vmin=vmin_jul,vmax=vmax_jul)\n",
    "\n",
    "Plot_text = 'Estimates'\n",
    "fig, ax = plt.subplots(2,3,figsize=(16,6), subplot_kw = {'projection':ccrs.PlateCarree()})\n",
    "ax[0,0].add_feature(cfeature.COASTLINE)\n",
    "ax[0,0].add_feature(cfeature.BORDERS)\n",
    "ax[0,0].add_feature(cfeature.STATES)\n",
    "mesh = ax[0,0].pcolormesh(lon_grid,lat_grid, mean_shat_jul,cmap='viridis', norm=norm_jul,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[0,0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[0,0].set_title('Estimates July umols/m2s' + '(truth = ' + truth + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[0,0])\n",
    "\n",
    "Plot_text = 'Prior'\n",
    "#mesh_grid = mean_truth_grid\n",
    "ax[0,1].add_feature(cfeature.COASTLINE)\n",
    "ax[0,1].add_feature(cfeature.BORDERS)\n",
    "ax[0,1].add_feature(cfeature.STATES)\n",
    "mesh = ax[0,1].pcolormesh(lon_grid,lat_grid, mean_prior_array_jul,cmap='viridis', norm=norm_jul,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[0,1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[0,1].set_title(Plot_text+' July umols/m2s' + '(prior = ' + prior + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[0,1])\n",
    "\n",
    "Plot_text = 'Truth'\n",
    "#mesh_grid = mean_truth_grid\n",
    "ax[0,2].add_feature(cfeature.COASTLINE)\n",
    "ax[0,2].add_feature(cfeature.BORDERS)\n",
    "ax[0,2].add_feature(cfeature.STATES)\n",
    "mesh = ax[0,2].pcolormesh(lon_grid,lat_grid, mean_truth_array_jul,cmap='viridis', norm=norm_jul,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[0,2],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[0,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[0,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[0,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[0,2].set_title(Plot_text+' July umols/m2s' + '(prior = ' + prior + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[0,2])\n",
    "\n",
    "Plot_text = 'Estimates'\n",
    "vmin_feb = np.percentile(flatten_shat_feb, 10)\n",
    "vmax_feb = np.percentile(flatten_shat_feb, 90)\n",
    "norm_feb = Normalize(vmin=vmin_jul,vmax=vmax_jul)\n",
    "\n",
    "ax[1,0].add_feature(cfeature.COASTLINE)\n",
    "ax[1,0].add_feature(cfeature.BORDERS)\n",
    "ax[1,0].add_feature(cfeature.STATES)\n",
    "mesh = ax[1,0].pcolormesh(lon_grid,lat_grid, mean_shat_feb,cmap='viridis', norm=norm_feb,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[1,0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[1,0].set_title('Estimates Feb umols/m2s' + '(truth = ' + truth + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[1,0])\n",
    "\n",
    "Plot_text = 'Prior'\n",
    "ax[1,1].add_feature(cfeature.COASTLINE)\n",
    "ax[1,1].add_feature(cfeature.BORDERS)\n",
    "ax[1,1].add_feature(cfeature.STATES)\n",
    "mesh = ax[1,1].pcolormesh(lon_grid,lat_grid, mean_prior_array_feb,cmap='viridis', norm=norm_feb,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[1,1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[1,1].set_title(Plot_text+' Feb umols/m2s' + '(prior = ' + prior + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[1,1])\n",
    "\n",
    "Plot_text = 'Truth'\n",
    "ax[1,2].add_feature(cfeature.COASTLINE)\n",
    "ax[1,2].add_feature(cfeature.BORDERS)\n",
    "ax[1,2].add_feature(cfeature.STATES)\n",
    "mesh = ax[1,2].pcolormesh(lon_grid,lat_grid, mean_truth_array_feb,cmap='viridis', norm=norm_feb,shading='auto',transform=ccrs.PlateCarree())\n",
    "gdf_baltimore.plot(ax=ax[1,2],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "twr_gdf_NEB.plot(ax=ax[1,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_HAL.plot(ax=ax[1,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "twr_gdf_NWB.plot(ax=ax[1,2],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "ax[1,2].set_title(Plot_text+' Feb umols/m2s' + '(prior = ' + prior + ')', size = 10)\n",
    "fig.colorbar(mesh, ax=ax[1,2])\n",
    "\n",
    "plt.subplots_adjust(wspace=0.01)\n",
    "plt.show() \n",
    "\n",
    "vmin_jul_prior = np.percentile(diff_prior_jul, 10)\n",
    "vmax_jul_prior = np.percentile(diff_prior_jul, 90)\n",
    "norm_jul_prior = Normalize(vmin=vmin_jul_prior,vmax=vmax_jul_prior)\n",
    "\n",
    "if not Exercise1:\n",
    "    Plot_text = '[Estimates - Prior]'\n",
    "    fig, ax = plt.subplots(2,2,figsize=(8,10), subplot_kw = {'projection':ccrs.PlateCarree()})\n",
    "    ax[0,0].add_feature(cfeature.COASTLINE)\n",
    "    ax[0,0].add_feature(cfeature.BORDERS)\n",
    "    ax[0,0].add_feature(cfeature.STATES)\n",
    "    mesh = ax[0,0].pcolormesh(lon_grid,lat_grid, diff_prior_jul,cmap='viridis', norm=norm_jul_prior,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[0,0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[0,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[0,0].set_title(Plot_text+' July umol/m2s', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[0,0])\n",
    "\n",
    "    vmin_jul_truth = np.percentile(diff_truth_jul, 10)\n",
    "    vmax_jul_truth = np.percentile(diff_truth_jul, 90)\n",
    "    norm_jul_truth = Normalize(vmin=vmin_jul_truth,vmax=vmax_jul_truth)\n",
    "    Plot_text = '[Estimates - Truth]'\n",
    "    ax[0,1].add_feature(cfeature.COASTLINE)\n",
    "    ax[0,1].add_feature(cfeature.BORDERS)\n",
    "    ax[0,1].add_feature(cfeature.STATES)\n",
    "    mesh = ax[0,1].pcolormesh(lon_grid,lat_grid, diff_truth_jul,cmap='viridis', norm=norm_jul_truth,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[0,1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[0,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[0,1].set_title(Plot_text+' July umol/m2s', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[0,1])\n",
    "\n",
    "    vmin_feb_prior = np.percentile(diff_prior_feb, 10)\n",
    "    vmax_feb_prior = np.percentile(diff_prior_feb, 90)\n",
    "    norm_feb_prior = Normalize(vmin=vmin_feb_prior,vmax=vmax_feb_prior)\n",
    "    Plot_text = '[Estimates - Prior]'\n",
    "    ax[1,0].add_feature(cfeature.COASTLINE)\n",
    "    ax[1,0].add_feature(cfeature.BORDERS)\n",
    "    ax[1,0].add_feature(cfeature.STATES)\n",
    "    mesh = ax[1,0].pcolormesh(lon_grid,lat_grid, diff_prior_feb,cmap='viridis', norm=norm_feb_prior,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[1,0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[1,0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[1,0].set_title(Plot_text+' Feb umol/m2s', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[1,0])\n",
    "\n",
    "    vmin_feb_truth = np.percentile(diff_truth_feb, 10)\n",
    "    vmax_feb_truth = np.percentile(diff_truth_feb, 90)\n",
    "    norm_feb_truth = Normalize(vmin=vmin_feb_truth,vmax=vmax_feb_truth)\n",
    "    Plot_text = '[Estimates - Truth]'\n",
    "    ax[1,1].add_feature(cfeature.COASTLINE)\n",
    "    ax[1,1].add_feature(cfeature.BORDERS)\n",
    "    ax[1,1].add_feature(cfeature.STATES)\n",
    "    mesh = ax[1,1].pcolormesh(lon_grid,lat_grid, diff_truth_feb,cmap='viridis', norm=norm_feb_truth,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[1,1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[1,1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[1,1].set_title(Plot_text+ ' Feb umol/m2s', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[1,1])\n",
    "    plt.show()   \n",
    "\n",
    "print (' ')\n",
    "print ('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21899466-52bb-4b9c-831c-ff2a6d1debd6",
   "metadata": {},
   "source": [
    "#### If you estimated poseterior uncertanties, let's check out the reduction in uncertainty across our entire domain.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5e303-285e-465e-85d1-a6c637264a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if unc:\n",
    "    #%% Uncertainty reduction for both Feb and July\n",
    "    mesh_grid = np.sqrt(mean_q_diag_array_jul) - np.sqrt(mean_vshat_diag_jul)\n",
    "    vshat_ave = np.sqrt(np.mean(mean_vshat_diag_jul))\n",
    "    q_ave = np.sqrt(np.mean(mean_q_diag_array_jul))\n",
    "    print('Average So (Jul) = ' + str(round(q_ave,4)))\n",
    "    print('Average posterior uncertainty (Jul) = ' + str(round(vshat_ave,4)))\n",
    "    vmin = np.percentile(mesh_grid.flatten(), 1)\n",
    "    vmax = np.percentile(mesh_grid.flatten(), 99)\n",
    "    norm = Normalize(vmin=vmin,vmax=vmax)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize=(8,10), subplot_kw = {'projection':ccrs.PlateCarree()})\n",
    "    ax[0].add_feature(cfeature.COASTLINE)\n",
    "    ax[0].add_feature(cfeature.BORDERS)\n",
    "    ax[0].add_feature(cfeature.STATES)\n",
    "    mesh = ax[0].pcolormesh(lon_grid,lat_grid, np.sqrt(mesh_grid),cmap='viridis', norm=norm,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[0],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[0],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[0].set_title('Uncertainty Reduction Jul', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[0],shrink=0.5)\n",
    "\n",
    "    mesh_grid = np.sqrt(mean_q_diag_array_feb) - np.sqrt(mean_vshat_diag_feb)\n",
    "    vshat_ave = np.sqrt(np.mean(mean_vshat_diag_jul))\n",
    "    q_ave = np.sqrt(np.mean(mean_q_diag_array_jul))\n",
    "    print('Average So (Feb) = ' + str(round(q_ave,4)))\n",
    "    print('Average posterior uncertainty (Feb) = ' + str(round(vshat_ave,4)))\n",
    "    vmin = np.percentile(mesh_grid.flatten(), 1)\n",
    "    vmax = np.percentile(mesh_grid.flatten(), 99)\n",
    "    norm = Normalize(vmin=vmin,vmax=vmax)\n",
    "\n",
    "    ax[1].add_feature(cfeature.COASTLINE)\n",
    "    ax[1].add_feature(cfeature.BORDERS)\n",
    "    ax[1].add_feature(cfeature.STATES)\n",
    "    mesh = ax[1].pcolormesh(lon_grid,lat_grid, np.sqrt(mesh_grid),cmap='viridis', norm=norm,shading='auto',transform=ccrs.PlateCarree())\n",
    "    gdf_baltimore.plot(ax=ax[1],color='black',edgecolor = 'black', alpha=0.3,transform=ccrs.PlateCarree())\n",
    "    twr_gdf_NEB.plot(ax=ax[1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_HAL.plot(ax=ax[1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    twr_gdf_NWB.plot(ax=ax[1],marker='o', markersize=20, linewidth=0.5, color='red')\n",
    "    ax[1].set_title('Uncertainty Reduction Feb', size = 10)\n",
    "    fig.colorbar(mesh, ax=ax[1],shrink=0.5)\n",
    "    plt.show()  \n",
    "    print(' ')\n",
    "    print('done!')\n",
    "else:\n",
    "    print('Did not estimate uncertainties.  No map.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc2187-6276-4311-a7f6-4b060bcaade0",
   "metadata": {},
   "source": [
    "## Back to Research Question: \n",
    "### What are the February and July fossil CO<sub>2</sub> estimates for Baltimore in 2019?  Is our understanding of Baltimore's CO<sub>2</sub> emissions consistent with atmospheric observations?   \n",
    "\n",
    "- And we want to show in units that people can understand (not just the scientists!).  \n",
    "- And we want to compare statistics in this domain compared to the statistics for our entire estimation domain.  \n",
    "\n",
    "##### **Note:** a lot of **UGLY** plotting code in this block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb7752-c79c-4402-a5e3-afb2bfcdfd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Starting bar plot')\n",
    "print('Takes a second ...')\n",
    "\n",
    "lon_2d, lat_2d = np.meshgrid(lon_grid, lat_grid)\n",
    "points = [Point(xy) for xy in zip(lon_2d.ravel(), lat_2d.ravel())]\n",
    "grid_gdf = gpd.GeoDataFrame(geometry=points, crs=gdf_baltimore.crs)\n",
    "mask_Balt = grid_gdf.within(gdf_baltimore.unary_union).values.reshape(lon_2d.shape)\n",
    "\n",
    "days_feb, days_jul = 28, 31\n",
    "sec_per_day = 86400\n",
    "\n",
    "def flux_to_GgC(mean_flux, area_grid, days):\n",
    "    mol_per_m2_s = mean_flux * 1e-6\n",
    "    g_per_m2_s = mol_per_m2_s * 12.01\n",
    "    g_per_cell_s = g_per_m2_s * area_grid\n",
    "    g_per_cell = g_per_cell_s * days * sec_per_day\n",
    "    return g_per_cell.sum() / 1e9\n",
    "\n",
    "def std_flux_to_GgC(std_flux, area_grid, days):\n",
    "    std_flux = std_flux**2\n",
    "    mol_per_m2_s = std_flux * 1e-6\n",
    "    g_per_m2_s = mol_per_m2_s * 12.01\n",
    "    g_per_cell_s = g_per_m2_s * area_grid\n",
    "    g_per_cell = g_per_cell_s * days * sec_per_day\n",
    "    return np.sqrt(g_per_cell).sum() / 1e9\n",
    "\n",
    "#Prior mean arrays\n",
    "prior_feb = np.where(mask_Balt, mean_prior_array_feb, 0)\n",
    "prior_jul = np.where(mask_Balt, mean_prior_array_jul, 0)\n",
    "\n",
    "#Shat arrays\n",
    "post_feb = np.where(mask_Balt, mean_shat_feb, 0)\n",
    "post_jul = np.where(mask_Balt, mean_shat_jul, 0)\n",
    "\n",
    "truth_feb = np.where(mask_Balt, mean_truth_array_feb, 0)\n",
    "truth_jul = np.where(mask_Balt, mean_truth_array_jul, 0)\n",
    "\n",
    "#Unc arrays\n",
    "post_std_feb = np.where(mask_Balt, np.sqrt(mean_vshat_diag_feb), 0)\n",
    "post_std_jul = np.where(mask_Balt, np.sqrt(mean_vshat_diag_jul), 0)\n",
    "\n",
    "# Prior std arrays\n",
    "prior_std_feb = np.where(mask_Balt, np.sqrt(mean_q_diag_array_feb), 0)\n",
    "prior_std_jul = np.where(mask_Balt, np.sqrt(mean_q_diag_array_jul), 0)\n",
    "\n",
    "R_earth = 6.371e6\n",
    "dlat = np.radians(lat_grid[1] - lat_grid[0])  # radians\n",
    "dlon = np.radians(lon_grid[1] - lon_grid[0])  # radians\n",
    "lon_2d, lat_2d = np.meshgrid(lon_grid, lat_grid)\n",
    "area_grid = (R_earth**2) * dlat * dlon * np.cos(np.radians(lat_2d))  # in m^2\n",
    "\n",
    "prior_feb_GgC = flux_to_GgC(prior_feb, area_grid, days_feb)\n",
    "prior_jul_GgC = flux_to_GgC(prior_jul, area_grid, days_jul)\n",
    "\n",
    "post_feb_GgC = flux_to_GgC(post_feb, area_grid, days_feb)\n",
    "post_jul_GgC = flux_to_GgC(post_jul, area_grid, days_jul)\n",
    "\n",
    "truth_feb_GgC = flux_to_GgC(truth_feb, area_grid, days_feb)\n",
    "truth_jul_GgC = flux_to_GgC(truth_jul, area_grid, days_jul)\n",
    "\n",
    "post_unc_feb_GgC = std_flux_to_GgC(post_std_feb, area_grid, days_feb)\n",
    "post_unc_jul_GgC = std_flux_to_GgC(post_std_jul, area_grid, days_jul)\n",
    "\n",
    "prior_unc_feb_GgC = std_flux_to_GgC(prior_std_feb, area_grid, days_feb)\n",
    "prior_unc_jul_GgC = std_flux_to_GgC(prior_std_jul, area_grid, days_jul)\n",
    "\n",
    "labels = ['February', 'July']\n",
    "prior_vals_Balt = [prior_feb_GgC, prior_jul_GgC]\n",
    "post_vals_Balt = [post_feb_GgC, post_jul_GgC]\n",
    "truth_vals_Balt = [truth_feb_GgC, truth_jul_GgC]\n",
    "post_uncs_Balt = [post_unc_feb_GgC, post_unc_jul_GgC]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.5\n",
    "fig, ax = plt.subplots(figsize=(5,6))\n",
    "\n",
    "# Prior\n",
    "ax.bar(x, prior_vals_Balt, width/3,capsize = 3, label='Prior', color='lightgray')\n",
    "# Posterior with error bars\n",
    "if unc: \n",
    "    ax.bar(x-width/3, post_vals_Balt, width/3, yerr=(x*2 for x in post_uncs_Balt), capsize=3, label='Posterior', color='cornflowerblue')\n",
    "else:\n",
    "    ax.bar(x-width/3, post_vals_Balt, width/3, capsize=3, label='Posterior', color='cornflowerblue')\n",
    "# Truth\n",
    "ax.bar(x+width/3, truth_vals_Balt, width/3, label='Truth', color='forestgreen')\n",
    "\n",
    "ax.set_ylabel('Emissions (Gg C per month)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.title('Baltimore Emissions (Prior, Posterior, Truth)')\n",
    "ax.grid(True,zorder=0)\n",
    "ax.set_axisbelow(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# More stats for that area\n",
    "\n",
    "# Flatten all arrays\n",
    "flat_shat_feb = mean_shat_feb.flatten()\n",
    "flat_truth_feb = mean_truth_array_feb.flatten()\n",
    "flat_prior_feb = mean_prior_array_feb.flatten()\n",
    "flat_shat_jul = mean_shat_jul.flatten()\n",
    "flat_truth_jul = mean_truth_array_jul.flatten()\n",
    "flat_prior_jul = mean_prior_array_jul.flatten()\n",
    "\n",
    "mask_Balt_flat = mask_Balt.flatten()\n",
    "\n",
    "# Apply mask\n",
    "shat_feb_Balt = flat_shat_feb[mask_Balt_flat]\n",
    "truth_feb_Balt = flat_truth_feb[mask_Balt_flat]\n",
    "prior_feb_Balt = flat_prior_feb[mask_Balt_flat]\n",
    "shat_jul_Balt = flat_shat_jul[mask_Balt_flat]\n",
    "truth_jul_Balt = flat_truth_jul[mask_Balt_flat]\n",
    "prior_jul_Balt = flat_prior_jul[mask_Balt_flat]\n",
    "\n",
    "def calculate_rmse(truth, estimated):\n",
    "    return np.sqrt(np.nanmean((truth-estimated)**2))\n",
    "\n",
    "#February\n",
    "mean_truth_feb_Balt = np.mean(truth_feb_Balt)\n",
    "mean_diff_feb_Balt = np.mean(shat_feb_Balt - truth_feb_Balt)\n",
    "rmse_feb_Balt = calculate_rmse(truth_feb_Balt, shat_feb_Balt)\n",
    "corr_coef_feb_Balt = np.corrcoef(truth_feb_Balt, shat_feb_Balt)[0,1]\n",
    "std_err_feb_Balt = np.std(shat_feb_Balt - truth_feb_Balt)/np.sqrt(len(truth_feb_Balt))\n",
    "\n",
    "#July\n",
    "mean_truth_jul_Balt = np.mean(truth_jul_Balt)\n",
    "mean_diff_jul_Balt = np.mean(shat_jul_Balt - truth_jul_Balt)\n",
    "rmse_jul_Balt = calculate_rmse(truth_jul_Balt, shat_jul_Balt)\n",
    "corr_coef_jul_Balt = np.corrcoef(truth_jul_Balt, shat_jul_Balt)[0,1]\n",
    "std_err_jul_Balt = np.std(shat_jul_Balt - truth_jul_Balt)/np.sqrt(len(truth_jul_Balt))\n",
    "\n",
    "shat_mean_feb_Balt = np.mean(shat_feb_Balt)\n",
    "shat_mean_jul_Balt = np.mean(shat_jul_Balt)\n",
    "\n",
    "shat_mean_feb_Balt = np.mean(shat_feb_Balt)\n",
    "shat_mean_jul_Balt = np.mean(shat_jul_Balt)\n",
    "\n",
    "prior_mean_feb_Balt = np.mean(prior_feb_Balt)\n",
    "prior_mean_jul_Balt = np.mean(prior_jul_Balt)\n",
    "\n",
    "print(' ')\n",
    "print('Statistics')\n",
    "print(' ')\n",
    "print('Mean truth flux (Feb):' + str(round(mean_truth_feb_Balt,3)) + ' µmol/m2s')\n",
    "print('Mean truth flux (Jul):' + str(round(mean_truth_jul_Balt,3)) + ' µmol/m2s')\n",
    "print('')\n",
    "print('Mean prior flux (Feb):' + str(round(prior_mean_feb_Balt,3)) + ' µmol/m2s')\n",
    "print('Mean prior flux (Jul):' + str(round(prior_mean_jul_Balt,3)) + ' µmol/m2s')\n",
    "print(' ')\n",
    "print('Mean estimated flux (Feb):' + str(round(shat_mean_feb_Balt,3)) + ' µmol/m2s')\n",
    "print('Mean estimated flux (Jul):' + str(round(shat_mean_jul_Balt,3)) + ' µmol/m2s')\n",
    "print('')\n",
    "print(f\"Mean difference (xhat - xtruth) Feb (Baltimore): {mean_diff_feb_Balt:.4f} µmol/m2s\")\n",
    "print(f\"Mean difference (xhat - xtruth) Jul (Baltimore): {mean_diff_jul_Balt:.4f} µmol/m2s\")\n",
    "print('')\n",
    "print(f\"Std Error Feb (Baltimore): {std_err_feb_Balt:.4f} µmol/m2s\")\n",
    "print(f\"Std Error Jul (Baltimore): {std_err_jul_Balt:.4f} µmol/m2s\")\n",
    "print(' ')\n",
    "print(f\"Correlation Coefficient (xhat,xtruth) Feb (Baltimore): {corr_coef_feb_Balt:.4f}\")\n",
    "print(f\"Correlation Coefficient (xhat,xtruth) Jul (Baltimore): {corr_coef_jul_Balt:.4f}\")\n",
    "print(' ')\n",
    "print(f\"RMSE Feb (Baltimore): {rmse_feb_Balt:.4f} µmol/m2s\")\n",
    "print(f\"RMSE Jul (Baltimore): {rmse_jul_Balt:.4f} µmol/m2s\")\n",
    "\n",
    "print(' ')\n",
    "print('done')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
