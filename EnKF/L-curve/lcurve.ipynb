{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecc4c8a2-26b2-42a0-828f-24ce4acb17ac",
   "metadata": {},
   "source": [
    "# The L-Curve\n",
    "\n",
    "Due both to the sparseness of the global observing network, and also to the efficiency of atmospheric mixing at erasing gradients arising from\u0005surface fluxes, the atmospheric inversion problem is frequently underdetermined.  In the face of such ill-posed inverse problems, flux dipoles and associated artifacts arise, and smoothing techniques (such as Bayesian blending with priors) are often used, not to apply independent knowledge, but to \"regularize\" the solution...to make it look better.\n",
    "\n",
    "Underconstrained problems without regularization are characterized by highly variable posterior fluxes, often with strong off-diagonal elements in the posterior covariance matrix. Regularizing the solution reduces the variance of the estimated fluxes.  This smoothing works by attenuating spurious fluxes arising from overfitting to observations.   Regularized results are smoother and usually more realistic, but they do not agree with observations as well.  The balance between realistic fluxes and small observational residuals is determined by the relative sizes of observation error $\\mathbf{S_z}$ and prior flux error $\\mathbf{S_x}$.  This Bayesian balance is expressed by the two terms of a typical cost function: \n",
    "\n",
    "$$J = (\\mathbf{z} - \\mathbf{H}\\mathbf{x})^\\text{T} \\mathbf{S_z}^{-1} (\\mathbf{z}-\\mathbf{H}\\mathbf{x}) + (\\mathbf{x}-\\mathbf{x_0})^\\text{T} \\mathbf{S_x}^{-1} (\\mathbf{x}-\\mathbf{x_0})$$\n",
    "\n",
    "In many ill-posed problems where the prior flux error is not known, an optimal Bayesian balance can be found empirically.  When the flux variance is plotted against the residuals variance as a function of the amount of regularization, a characteristic L-shaped curve often emerges.  The optimal solution is at the elbow of the L-curve, where most spurious fluxes have been attenuated without introducing significant residuals.\n",
    "\n",
    "![Hansen L-curve image](hansen98a-fig4.1.png \"Hansen 1998\")\n",
    "\n",
    "We vary the balance between the two parts of the cost function (matching measurements vs. matching priors) by scaling the prior covariance $\\mathbf{S_x}$ by a factor we call \"$\\alpha$\". In general, this balance is maintained by the magnitudes of $\\mathbf{S_x}$ and $\\mathbf{S_z}$. The L-curve diagnostic is useful in finding an optimal balance between these terms.\n",
    "\n",
    "$$J = (\\mathbf{z} - \\mathbf{H}\\mathbf{x})^\\text{T} \\mathbf{S_z}^{-1} (\\mathbf{z}-\\mathbf{H}\\mathbf{x}) + (\\mathbf{x}-\\mathbf{x_0})^\\text{T} (\\alpha \\mathbf{S_x})^{-1} (\\mathbf{x}-\\mathbf{x_0})$$\n",
    "\n",
    "In this particular case, we know that $\\alpha = 1$ is the correct scaling of the prior covariance, since that is what we use to generate the truth condition and measurement data. You will see that this case lies at or near the \"elbow\" of the L-curve. Details depend on the random selection of both the truth condition and the observational network. Some pathological cases can occur in which results do not fall along the expected L shape.\n",
    "\n",
    "## References\n",
    "P. C. Hansen. Rank-deficient and discrete ill-posed problems: numerical aspects of linear inversion. SIAM, 1998.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a3e6b-7bfd-4d73-815a-790965cf4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-stamp: <aj:/Users/andy/Desktop/ssim-ghg/EnKF/L-curve/lcurve.r - 05 Feb 2025 (Wed) 19:18:38 MST>\n",
    "\n",
    "source(\"../tools/enkf.r\")\n",
    "source(\"../tools/progress.bar.r\")\n",
    "source(\"../tools/find.indir.r\")\n",
    "indir <- find.indir() # potentially replace with trivial yaml access\n",
    "\n",
    "# explicitly include 1.0 since for this artificial \n",
    "# case, it is the perfect configuration\n",
    "alphas <-unique(sort(c(1,10^seq(-3,3,length.out=40))))\n",
    "\n",
    "nalphas <- length(alphas)\n",
    "\n",
    "# allocate storage for results\n",
    "var.resids <- matrix(NA,nrow=1,ncol=nalphas)\n",
    "var.flux <- matrix(NA,nrow=1,ncol=nalphas)\n",
    "                                      \n",
    "# Load sensitivity matrices (Jacobians)\n",
    "if(!exists(\"H.orig\")) {\n",
    "    t0 <- proc.time()[3]\n",
    "    cat(\"Loading Jacobians...\")\n",
    "    H.orig <- list()\n",
    "    load(file.path(indir,\"jacobians/trunc_full_jacob_030624_with_dimnames_sib4_4x5_mask.rda\"))\n",
    "    load(file.path(indir,\"jacobians/jacob_bgd_021624.rda\"))\n",
    "    H.orig$H <- jacob*(12/44) # Andrew reports units conversion needed\n",
    "    H.orig$H_fixed <- jacob_bgd[,c(2,3)]\n",
    "    rm(jacob,jacob_bgd)\n",
    "    cat(sprintf('%.1fs\\n',proc.time()[3]-t0))\n",
    "}\n",
    "\n",
    "nobs <- dim(H.orig$H)[1]\n",
    "\n",
    "Szd.actual <- rep(0.5,nobs) # variance in ppm^2\n",
    "Szd.assumed <- Szd.actual\n",
    "\n",
    "nparms <- 22*24 # 22 regions, 24 months\n",
    "\n",
    "# Real covariance of unknowns\n",
    "Sx <- diag(rep(1,nparms))\n",
    "\n",
    "# The generate_ensemble() function just chooses nmemb samples from \n",
    "# the multivariate normal distribution with covariance Sx and mean 0.\n",
    "# We repurpose this to choose a random truth condition here.\n",
    "truth_condition <- generate_ensemble(Sx=Sx,nmemb=1)\n",
    "dim(truth_condition) <- c(nparms,1)\n",
    "\n",
    "\n",
    "# Perturbed observations (because Szd is supplied)\n",
    "obs <- simulate_observed(H=H.orig$H, x=truth_condition, H_fixed=H.orig$H_fixed, Szd=Szd.actual)\n",
    "dim(obs) <- c(nobs,1)\n",
    "\n",
    "# Restrict to nobs randomly sampled subset of measurements. Could use\n",
    "# obs_catalog or row.names of H to do more systematically-chosen\n",
    "# subsets.\n",
    "#\n",
    "# The L-curve is typical of /under/-constrained problems, so we\n",
    "# restrict to just a small number of observations.\n",
    "nobs <- 200\n",
    "lx <- sample(x=1:length(obs),size=nobs)\n",
    "obs <- obs[lx,]\n",
    "Szd.assumed <- Szd.assumed[lx]\n",
    "Szd.actual <- Szd.actual[lx]\n",
    "H <- H.orig$H[lx,]\n",
    "H_fixed <- H.orig$H_fixed[lx,]\n",
    "\n",
    "obs_fixed <- apply(H_fixed,c(1),sum)\n",
    "\n",
    "pb <- progress.bar.start(nalphas)\n",
    "\n",
    "ialpha <- 0\n",
    "for (alpha in alphas) {\n",
    "  ialpha <- ialpha + 1\n",
    "\n",
    "  Sx.prior <- alpha*Sx \n",
    "  x.prior <- rep(0,nparms)\n",
    "  \n",
    "  \n",
    "  # Kalman filter measurement update\n",
    "  kf <- kf_meas_update(x=x.prior,Sx=Sx.prior,H=H,z=obs-obs_fixed,\n",
    "                       Sz=diag(Szd.assumed))\n",
    "  \n",
    "  dobs <- matrix(simulate_observed(H=H, x=kf$x,H_fixed=H_fixed) - obs,nrow=nobs,ncol=1)\n",
    "  dx <- matrix(kf$x - x.prior,nrow=nparms,ncol=1)\n",
    "  \n",
    "  var.resids[ialpha] <- var(as.vector(dobs))\n",
    "  var.flux[ialpha] <- var(as.vector(kf$x))\n",
    "\n",
    "  pb <- progress.bar.print(pb,ialpha)\n",
    "} # alpha\n",
    "\n",
    "progress.bar.end(pb)\n",
    "\n",
    "# save(chi2.resids,chi2.flux,var.flux,alphas,file=\"lcurve.rda\")\n",
    "\n",
    "options(jupyter.plot_scale=1,repr.plot.height=7,repr.plot.width=14)\n",
    "layout(matrix(1:2,nrow=1))\n",
    "\n",
    "lx <- c(which.min(alphas),which(alphas==1),which.max(alphas))\n",
    "plot(x=var.resids, y=var.flux,xlab=\"variance of the obs resids\",ylab=\"variance of optimized parameters\",main=\"Linear scale\")\n",
    "points(x=var.resids[lx], y=var.flux[lx],pch=20,col='red')\n",
    "text(x=var.resids[lx], y=var.flux[lx],labels=alphas[lx],pos=4,xpd=NA)\n",
    "\n",
    "plot(x=var.resids, y=var.flux,xlab=\"variance of the obs resids\",ylab=\"variance of optimized parameters\",log='xy',main=\"Log scale\")\n",
    "text(x=var.resids[lx], y=var.flux[lx],labels=alphas[lx],pos=4,xpd=NA)\n",
    "points(x=var.resids[lx], y=var.flux[lx],pch=20,col='red')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b2f29bd-5c08-4ed6-b838-34dcf7cb83d5",
   "metadata": {},
   "source": [
    "The Transcom 3, level 1 (annual mean) inversions of Gurney et al. (2002) chose a weighting of the priors which was almost optimal. It favored agreement with measurements over a smoother solution. Different models (shown as different colored lines) have different L curves due to their differing transport, probably the strength of their mixing.\n",
    "\n",
    "![Transcom3 level 1 L-curve image](t3l1.lcurve.png \"Transcom3 level 1\")\n",
    "\n",
    "K. R. Gurney, R. M. Law, A. S. Denning, P. J. Rayner, D. Baker, P. Bousquet, L. Bruhwiler, Y.-H. Chen, P. Ciais, S. Fan, I. Y. Fung, M. Gloor, M. Heimann, K. Higuchi, J. John, T. Maki, S. Maksyutov, K. Masarie, P. Peylin, M. Prather, B. Pak, J. Randerson, J. L. Sarmiento, S. Taguchi, T. Takahashi, P. Tans, and C.-W. Yuen. Towards robust regional estimates of CO2 sources and sinks using atmospheric transport models. Nature, 415, 2002."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
